---
title: "Assignment 2 Report"
author: "Saurabh Giram"
date: "`r Sys.Date()`"
output:
  word_document:
    toc: yes
    toc_depth: '3'
  pdf_document:
    fig_caption: yes
    citation_package: natbib
    toc: yes
    toc_depth: 3
    number_sections: yes
fontsize: 12pt
geometry: margin=2.0cm
---

\newpage # adds new page after title
\tableofcontents # adds table of contents
\listoffigures
\listoftables
\newpage

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

The data set associated with ATM transactions happened during the period 18 Mar 1996 - 22 Mar 1998 in England.As in the last assignment we explore the time series of the data set NN5-093 and did describe the various finding associated with the exploration. We found out strong presence of weekly seasonality. Aim of this report is to highlight and finalizes the forecasting model applicable to the the data set "NN5-093". Aim in this process is to apply various models starting with seasonal naive, arithmetic mean, ETS, ARIMA, Neural network and finally Multiple regression models. I will compare the model accuracy with the out sample of the data set along with the rolling origin and after comparing various models based on this result, will apply most accurate model to the complete data set to forecast future 2 weeks data. Before we start processing with respect to forecasting, we need to critically analyse the data and make it consistent throughout the dataset so that forecasting processes could be done effectively. In the next section we address those and then we will progress with applying the forecasting processes.
 
```{r Library and Dataset import, include=FALSE}

#Importing libraries
library("forecast")
library("tsutils")
library("tseries")
library("readxl")
library("xts")
library("seastests")
library("tinytex")
library("tsibble")
library("dplyr")
library("outliers")
library("moments")
library("VIM")
library("naniar")
library("ggplot2")
library("imputeTS")
library("knitr")
library("regclass")


#Import data
dataset <- read_excel("Assignment 1 Data.xls")

#colnames(data) <- c("Date","Transactions")
colnames(dataset) <- c("Transactions")

#Converting data to time series
dataset <- ts(dataset, frequency = 7, start = c(1996,77))

# Histogram of whole distribution before cleaning the data set
hist(dataset)

```

## Time Series Components Analysis -

When we decompose this time series in previous report in detailed manner, we found out strong presence of weekly seasonality. One thing would like to highlight here, when we plotted the distribution of the complete data set we could see it is rightly skewed but not the normally distributed which could be bit difficult to further explore and analyse.Reason for rightly skewed could be because large presence of outliers plus NAs data. To lower the impact of season the current dataset we applied various methods and analyse what impact could it do to the output if we replace with one specific method. Below are the specifics -

```{r Histogram of dataset before modification, echo=FALSE}
hist(dataset)
```

### Handling missing data

We have 20 NA present in the data set, among those seems like 12 NA entries are from specifically when there was a Sunday and 5 are when there was a Friday. There seems be like a something is not usual on Sundays and Fridays.

```{r Missing values distribution, echo=FALSE}
# Missing values distribution
ggplot_na_distribution(dataset)
#ggplot_na_intervals(dataset)
#ggplot_na_gapsize(dataset)
```

I tried various different methods as na_seasplit, random, mean, median and na_interpolation and compare the mean values of data set after imputing in the data set with below methods and found out the na interpolation method has lowest mean after imputing the NAs with interpolation method and has lowest RMSE of irregular components found through decomposition function R compared with others. I believe lowest the RMSE of irregular components make better the time series as it will reduce errors in the dataset.  Hence we will go forward with na interpolation method. 

na_seasplit which is basically Splits the times series into seasons and afterwards performs imputation separately for each of the resulting time series data sets (each containing the data for one specific season)

```{r Handling missing values, include=FALSE}
#Filing missing values

# Approach 1 -Replace with corresponding day of week data
which_na(dataset)

# Approach 2 - Replace with interpolation method
dataset <- na_interpolation(dataset)

summary(dataset)
plot(dataset)
which_na(dataset)
```

```{r Summary after missing values, echo=FALSE}
summary(dataset)
```

### Handling Outliers

The dataset also consist of outliers along with missing values. Now the question which we will be discussing next hasnt been highlighted in previous findings.   
Now question arises, should we impute the missing values first with appropriate algorithm or should we handle the outliers first?
If we handle the outliers first by replacing with median value of data set, (as outliers values don't impact median) we will be able to reduce some of the noise and make the series better streamline.
But in this case we are introducing the bias element in the data set which may not turn reasonable down the line interns of forecasting. 
If we calculate the RMSE of the irregular components after first replacing the outliers with median value and then replacing the missing values with the interpolation method it would be around 4.12, but interestingly by visual inspection of decompose time series we can see that it added the lot of irregular components to the time series though reduced the RMSE.
If we replace the NA values first then handle the outliers with median values RSME of irregular components is 4.69 but it had smooth the outliers, we can clearly see in decompose plot.
In this later approach, while calculating the replacement for missing values its taking outliers value into consideration which essentially making the replacement value slightly greater but on positive side its not adding the bias to the time series which could be a good side of it.
So though the in approach 1 where replacement of outlier happens first and then missing values RMSE is less than compared to the other approach I will still prepare to go with other approach where replacement of missing value would happen first and then outlier because it will not add bias and nor smooth the irregular components and will be in taking consideration the actual time series instead of fitted outliers.

```{r Box Plot of Outliers before replacement, echo=FALSE}
# Box plot the outliers
boxplot(dataset)
# Check how many are the outliers
boxplot.stats(dataset)
```

```{r Handling outliers, include=FALSE}

# Statistical test for outliers
grubbs.test(dataset,type = 11, opposite = FALSE, two.sided = TRUE)
chisq.out.test(dataset)

# Find the outliers in the series
out <- boxplot.stats(dataset)$out 
out
out_ind <- which(dataset %in% c(out))
out_ind
dataset[c(163,167,196,235,265,283,538,557,558,559,561,562,563,565,586,587,592,593,594,600
                 ,621,628,649,670,684,691,698,711,712,726)]

#Replacing the outliers with median values 
series_median = median(dataset)
series_median
dataset[c(163,167,196,235,265,283,538,557,558,559,561,562,563,565,586,587,592,593,594,600
                 ,621,628,649,670,684,691,698,711,712,726)] = series_median


# Summary of dataset
summary(dataset)

```

After replacing the missing values with interpolation method and reducing the impact of outliers by replacing with median of the dataset, if we plot the distribution, we can see it is now normally distributed, so we are in better shape to proceed further.

```{r Box Plot after outliers replacement, echo=FALSE}

# Box plot the outliers after replacing previously identified outliers with series median
boxplot(dataset)
# Check how many are the outliers after replacing previously identified outliers with series median
boxplot.stats(dataset)
```


```{r Histogram of whole distribution, echo=FALSE}
hist(dataset) 

```

## Fitting the model -

## Preparing for train and test dataset -

To test different methods and models on the time series, we split the original dataset into a train set 
and test set. The dataset has 735 observations. In initial analysis we found that by taking the train dataset as a 70% of total dataset, result of application of forecasting methods is not consistent with the actual dataset in terms of finding the pattern in the series. For ex, if we take 70% dataset as a train dataset,we may not find multiplicative models as a appropriate models to this dataset but if we choose the train dataset as 95% of total (which is not ideal but as per the dataset behavior), its giving consistent result. Hence train dataset is having 721 observations and test dataset will be 14 observations.

```{r Preparing for train and test dataset, include=FALSE}
# Length of data set 
length(dataset)

# Frequency of dataset 
frequency(dataset)

# Visualize the time series of data sES
plot(dataset)

# Histogram of whole distribution
hist(dataset) 

#Test for trend
trendtest(dataset)

#Checking for seasonality
isSeasonal(dataset, freq = 365)
isSeasonal(dataset, freq= 7)
isSeasonal(dataset, test = "combined")

seastests::welch(dataset)

#Decomposition of the dataset 3 using decomp function
decomp_dataset <- decomp(dataset,outplot = TRUE)

# Plot the season
seasplot(dataset,m = 7)

#ACF and PACF analysis on the entire dataset
tsdisplay(dataset)

# Perform KPSS and ADF test on time series
kpss.test(dataset)
adf.test(dataset)

# Split into training and test --------------------------------------------

# Find the total number of observations
dataset_length <- length(dataset)
# Write down size of training set
dataset_train_length <- 721

# Split into training and test
dataset_train_length<- 721
dataset_train <- ts(dataset[1:dataset_train_length], frequency = 7)
length(dataset_train)
dataset_test <- dataset[(dataset_train_length+1):length(dataset)]
length(dataset_test)

#Setting the horizon
h=14
```

In order to correct gauge the accuracy of applied forecasting models I will be using rolling origin method where the origin will not be constant while producing the forecast. By assessing the result through rolling origin, i would assume we will get better picture interns of forecasting models behavior.

```{r Rolling origin, include=FALSE}
# Set horizon and number of rolling origins
H <- 14 #42 # 14 # 32
origins <- 14 #14
dataset_length <- length(dataset)
dataset_rolling_train_length <- dataset_length - H - origins + 1
dataset_rolling_test_length <- H + origins - 1
dataset_rolling_train <- ts(dataset[1:dataset_rolling_train_length],
                            frequency=frequency(dataset),
                            start=start(dataset))
dataset_rolling_test <- dataset[(dataset_rolling_train_length+1):dataset_length]


```

### Arithmetic Mean Model

We will start with Arithmetic Mean model which is basically uses recent value as a forecast value for a future. Below is the forecast plot using Arithmetic Mean Model -

```{r #Arithmetic mean model, include=FALSE}

#Arithmetic mean model
Arithmetic_mean_dataset<- mean(dataset_train)
Forecast_Arithmetic_mean_dataset<- forecast(Arithmetic_mean_dataset,h=h)$mean
Forecast_Arithmetic_mean_dataset
#Error Measures for Arithmetic mean model
Arithmetic_errors_dataset<- dataset_test - Forecast_Arithmetic_mean_dataset
Arithmetic_ME_dataset<- mean(Arithmetic_errors_dataset) #Mean error
Arithmetic_MSE_dataset<- mean(Arithmetic_errors_dataset^2) #Mean squared error
Arithmetic_MAE_dataset<- mean(abs(Arithmetic_errors_dataset)) #Mean absolute error
Arithmetic_MAPE_dataset<- 100 * mean(abs(Arithmetic_errors_dataset)/dataset_test) #Mean absolute percentage error
Arithmetic_RMSE_dataset<- sqrt(mean(Arithmetic_errors_dataset^2)) #Root mean squared error

## Rolling origin for Arithmetic mean

dataset_rolling_forecasts_Arithmeticmean <- matrix(NA, nrow=origins, ncol=H)
dataset_rolling_holdout_Arithmeticmean <- matrix(NA, nrow=origins, ncol=H)
colnames(dataset_rolling_forecasts_Arithmeticmean) <- paste0("horizon",c(1:H))
rownames(dataset_rolling_forecasts_Arithmeticmean) <- paste0("origin",c(1:origins))
dimnames(dataset_rolling_holdout_Arithmeticmean) <- dimnames(dataset_rolling_forecasts_Arithmeticmean)

for(i in 1:origins)
{
  # Create a ts object out of the dataset data
  dataset_rolling_train_set <- ts(dataset[1:(dataset_rolling_train_length+i-1)],
                                  frequency=frequency(dataset),
                                  start=start(dataset))
  
  # Write down the holdout values from the test set
  dataset_rolling_holdout_Arithmeticmean[i,] <- dataset_rolling_test[i-1+(1:H)]
  
  # Produce forecasts and write them down
  dataset_rolling_forecasts_Arithmeticmean[i,] <- forecast(mean(dataset_rolling_train_set),h=H)$mean
}

## MAPE for Rolling origin of Arithmetic mean
colMeans(abs(dataset_rolling_holdout_Arithmeticmean - dataset_rolling_forecasts_Arithmeticmean))
Rolling_errors_dataset_Arithmetic<- dataset_rolling_holdout_Arithmeticmean - dataset_rolling_forecasts_Arithmeticmean
Rolling_ME_errors_dataset_Arithmetic<- mean(Rolling_errors_dataset_Arithmetic) #Mean error
Rolling_MSE_errors_dataset_Arithmetic<- mean(Rolling_errors_dataset_Arithmetic^2) #Mean squared error
Rolling_MAE_errors_dataset_Arithmetic<- mean(abs(Rolling_errors_dataset_Arithmetic)) #Mean absolute error
Rolling_MAPE_dataset_Arithmetic<- 100 * mean(abs(Rolling_errors_dataset_Arithmetic)/dataset_rolling_holdout_Arithmeticmean)
Rolling_RMSE_errors_dataset_Arithmetic<- sqrt(mean(Rolling_errors_dataset_Arithmetic^2)) #Root mean squared error


```

```{r Plot of forecast using Arithmetic mean model, echo=FALSE}
# Plot the graph of error
plot(Forecast_Arithmetic_mean_dataset, xlab="Forecast", ylab="ATM Transactions",
     main="Forecast using Arithmetic mean model")
```

There are still some residuals present in error measured against the test data set and the forecast-ed values. 
If we plot the scatter plot of the error measured we could see it is hugely distributed and we can spot some data lying in the middle of the plot in linear fashion.

If we measure the various errors with respect to the test data set of this model we built on training dataset. Below is the summary -

```{r Summary table for error measure of Arithmetic Mean Model, include=FALSE}
# Create summary table for error measure of Arithmetic Mean Model

Summary_stats <- matrix(c(Arithmetic_ME_dataset,Arithmetic_MSE_dataset,Arithmetic_MAE_dataset,Arithmetic_MAPE_dataset,Arithmetic_RMSE_dataset), ncol=5, byrow=TRUE)
Summary_stats <- rbind(Summary_stats,c(Rolling_ME_errors_dataset_Arithmetic,Rolling_MSE_errors_dataset_Arithmetic,Rolling_MAE_errors_dataset_Arithmetic,Rolling_MAPE_dataset_Arithmetic,Rolling_RMSE_errors_dataset_Arithmetic))
colnames(Summary_stats) <- c('ME','MSE','MAE','MAPE','RMSE')
rownames(Summary_stats) <- c('Arithmetic Mean Model Error Measures','Arithmetic Mean Model Error Measures with Rolling origin')
Summary_stats <- as.table(Summary_stats)
names(Summary_stats) <- c("Arithmetic Mean Model Error Measures","Arithmetic Mean Model Error Measures with Rolling origin")
knitr::kable(Summary_stats)

```

### Simple Moving Average

We are using here simple moving average of order 8.
This is seems to be a better model than the previous arithmetic mean model.
There is no impact on ACF/PACF graph compared to the arithmetic mean
Residuals are greatly reduced in the SMA compared with the arithmetic mean method.
Below is the summary of the error measured

Forecast seems to be similar to what we observed in the data as seasonal. It could be a good fit for our data set.

```{r Simple Moving Average, include=FALSE}

#Fitting a Simple average model
SMA_dataset <- ma(dataset_train, order=8, centre=FALSE)

SMA_no_NAs_dataset <- SMA_dataset[!is.na(SMA_dataset)]
Forecast_SMA_dataset <- ts(rep(SMA_no_NAs_dataset[length(SMA_no_NAs_dataset)],h), frequency=7)

#Calculating the error measures for Simple average model
SMA_errors_dataset <- dataset_test - Forecast_SMA_dataset
SMA_ME_dataset <- mean(SMA_errors_dataset) #Mean error
SMA_MSE_dataset <- mean(SMA_errors_dataset^2) #Mean squared error
SMA_MAE_dataset<- mean(abs(SMA_errors_dataset)) #Mean absolute error
SMA_MAPE_dataset <- 100 * mean(abs(SMA_errors_dataset)/dataset_test) #Mean absolute percentage error
SMA_RMSE_dataset<- sqrt(mean(SMA_errors_dataset^2)) #Root mean squared error

## Rolling origin for SMA

dataset_rolling_forecasts_SMA <- matrix(NA, nrow=origins, ncol=H)
dataset_rolling_holdout_SMA <- matrix(NA, nrow=origins, ncol=H)
colnames(dataset_rolling_forecasts_SMA) <- paste0("horizon",c(1:H))
rownames(dataset_rolling_forecasts_SMA) <- paste0("origin",c(1:origins))
dimnames(dataset_rolling_holdout_SMA) <- dimnames(dataset_rolling_forecasts_SMA)

for(i in 1:origins)
{
  # Create a ts object out of the dataset data
  dataset_rolling_train_set <- ts(dataset[1:(dataset_rolling_train_length+i-1)],
                                  frequency=frequency(dataset),
                                  start=start(dataset))
  
  # Write down the holdout values from the test set
  dataset_rolling_holdout_SMA[i,] <- dataset_rolling_test[i-1+(1:H)]
  
  # Produce forecasts and write them down
  dataset_rolling_forecasts_SMA[i,] <- forecast(ma(dataset_rolling_train_set, order=8, centre=FALSE),h=H)$mean
}

## MAPE for Rolling origin of SMA
Rolling_errors_dataset_SMA<- dataset_rolling_holdout_SMA - dataset_rolling_forecasts_SMA
Rolling_ME_dataset_SMA<- mean(Rolling_errors_dataset_SMA) #Mean error
Rolling_MSE_dataset_SMA<- mean(Rolling_errors_dataset_SMA^2) #Mean squared error
Rolling_MAE_dataset_SMA<- mean(abs(Rolling_errors_dataset_SMA)) #Mean absolute error
Rolling_MAPE_dataset_SMA<- 100 * mean(abs(Rolling_errors_dataset_SMA)/dataset_rolling_holdout_SMA)
Rolling_RMSE_dataset_SMA<- sqrt(mean(Rolling_errors_dataset_SMA^2)) #Root mean squared error

```

### Seasonal Naive method

Naive method would be a good benchmark to start with.
There is no impact on ACF/PACF graph compared to the previous 2 methods
Below is the summary of the error measured

```{r Seasonal Naive, include=FALSE}
#Seasonal Naive model
SNaive_method_dataset <- snaive(dataset_train, h=h)
Forecast_Snaive_dataset <- SNaive_method_dataset$mean
plot(SNaive_method_dataset)
#Calculating the error measures for Naive
SNaive_errors_dataset<- dataset_test - Forecast_Snaive_dataset
SNaive_ME_dataset <- mean(SNaive_errors_dataset) #Mean error
SNaive_MSE_dataset <- mean(SNaive_errors_dataset^2) #Mean squared error
SNaive_MAE_dataset<- mean(abs(SNaive_errors_dataset)) #Mean absolute error
SNaive_MAPE_dataset <- 100 * mean(abs(SNaive_errors_dataset)/dataset_test) #Mean absolute percentage error
SNaive_RMSE_dataset<- sqrt(mean(SNaive_errors_dataset^2)) #Root mean squared error

## Rolling origin for SNaive

dataset_rolling_forecasts_SNaive <- matrix(NA, nrow=origins, ncol=H)
dataset_rolling_holdout_SNaive <- matrix(NA, nrow=origins, ncol=H)
colnames(dataset_rolling_forecasts_SNaive) <- paste0("horizon",c(1:H))
rownames(dataset_rolling_forecasts_SNaive) <- paste0("origin",c(1:origins))
dimnames(dataset_rolling_holdout_SNaive) <- dimnames(dataset_rolling_forecasts_SNaive)
for(i in 1:origins)
{
  # Create a ts object out of the dataset data
  dataset_rolling_train_set <- ts(dataset[1:(dataset_rolling_train_length+i-1)],
                                  frequency=frequency(dataset),
                                  start=start(dataset))
  
  # Write down the holdout values from the test set
  dataset_rolling_holdout_SNaive[i,] <- dataset_rolling_test[i-1+(1:H)]
  
  # Produce forecasts and write them down
  dataset_rolling_forecasts_SNaive[i,] <- snaive(dataset_rolling_train_set,h=H)$mean
}

## MAPE for Rolling origin of SNaive
Rolling_errors_dataset_SNaive<- dataset_rolling_holdout_SNaive - dataset_rolling_forecasts_SNaive
Rolling_ME_dataset_SNaive<- mean(Rolling_errors_dataset_SNaive) #Mean error
Rolling_MSE_dataset_SNaive<- mean(Rolling_errors_dataset_SNaive^2) #Mean squared error
Rolling_MAE_dataset_SNaive<- mean(abs(Rolling_errors_dataset_SNaive)) #Mean absolute error
Rolling_MAPE_dataset_SNaive<- 100 * mean(abs(Rolling_errors_dataset_SNaive)/dataset_rolling_holdout_SNaive)
Rolling_RMSE_dataset_SNaive<- sqrt(mean(Rolling_errors_dataset_SNaive^2)) #Root mean squared error

```

### Exponential Smoothing


#### Single Exponential Smoothing Method -

Here as the data has no trend but have the presence of strong seasonality, single exponential smoothing will not apply here.

#### Exponentail Smoothing Additive Error, No Trend and Additive Seasonality Model (A,N,A)

Based on the initial analysis we conclude that this dataset has no trend pattern. Based on this judgement below discussed model would be a good start to implement the exponential smoothing.
There is definitive seasonality component presence in the dataset so we would go with additive seasonality. 
Hence,additive Error, No Trend and Additive Seasonality Model (A,N,A) model could be a good benchmarking model to start the forecasting.

```{r ETS ANA, include=FALSE}
# Fit a model using ETS(A,N,A):
ETS_ANA_opt_dataset <- ets(dataset_train, model = "ANA")
# Check the AIC
ETS_ANA_opt_dataset
#Finding the coefficients
coef(ETS_ANA_opt_dataset)
#Forecasting the ANA model
Forecast_ETS_ANA_opt_dataset <- forecast(ETS_ANA_opt_dataset, h=h)
plot(Forecast_ETS_ANA_opt_dataset)

# Error check for Forecast for ETS(A,N,A)
ETS_ANA_opt_dataset_errors <- dataset_test - forecast(ETS_ANA_opt_dataset, h=h)$mean
ETS_ANA_ME_dataset<- mean(ETS_ANA_opt_dataset_errors) #Mean error
ETS_ANA_MSE_dataset<- mean(ETS_ANA_opt_dataset_errors^2) #Mean squared error
ETS_ANA_MAE_dataset<- mean(abs(ETS_ANA_opt_dataset_errors)) #Mean absolute error
ETS_ANA_MAPE_dataset<- 100 * mean(abs(ETS_ANA_opt_dataset_errors)/dataset_test) #Mean absolute percentage error
ETS_ANA_RMSE_dataset<- sqrt(mean(ETS_ANA_opt_dataset_errors^2)) # Root mean squared error

ETS_ANA_opt_dataset_errors
## Rolling origin for ETS ANA

dataset_rolling_forecasts_ETS_ANA <- matrix(NA, nrow=origins, ncol=H)
dataset_rolling_holdout_ETS_ANA <- matrix(NA, nrow=origins, ncol=H)
colnames(dataset_rolling_forecasts_ETS_ANA) <- paste0("horizon",c(1:H))
rownames(dataset_rolling_forecasts_ETS_ANA) <- paste0("origin",c(1:origins))
dimnames(dataset_rolling_holdout_ETS_ANA) <- dimnames(dataset_rolling_forecasts_ETS_ANA)

for(i in 1:origins)
{
  # Create a ts object out of the dataset data
  dataset_rolling_train_set <- ts(dataset[1:(dataset_rolling_train_length+i-1)],
                                  frequency=frequency(dataset),
                                  start=start(dataset))
  
  # Write down the holdout value Ts from the test set
  dataset_rolling_holdout_ETS_ANA[i,] <- dataset_rolling_test[i-1+(1:H)]
  
  # Produce forecasts and write them down
  dataset_rolling_forecasts_ETS_ANA[i,] <- forecast(ets(dataset_rolling_train_set,"ANA"),h=H)$mean
}

## MAPE for Rolling origin of ETA ANA

Rolling_errors_dataset_ETS_ANA<- dataset_rolling_holdout_ETS_ANA - dataset_rolling_forecasts_ETS_ANA
Rolling_ME_dataset_ETS_ANA<- mean(Rolling_errors_dataset_ETS_ANA) #Mean error
Rolling_MSE_dataset_ETS_ANA<- mean(Rolling_errors_dataset_ETS_ANA^2) #Mean squared error
Rolling_MAE_dataset_ETS_ANA<- mean(abs(Rolling_errors_dataset_ETS_ANA)) #Mean absolute error
Rolling_MAPE_dataset_ETS_ANA<- 100 * mean(abs(Rolling_errors_dataset_ETS_ANA)/dataset_rolling_holdout_ETS_ANA)
Rolling_RMSE_dataset_ETS_ANA<- sqrt(mean(Rolling_errors_dataset_ETS_ANA^2)) #Root mean squared error
```

#### Exponentail Smoothing Additive Error, Additive Trend and Additive Seasonality Model (A,A,A)

Though the dataset has very insignificant trend pattern it could be worth to apply the trend factor to consider the fact there trend pattern if consider the dataset has multi seasonality in it. This means that if this dataset consist of multi seasonality, i.e. yearly and wekkly, trend pattern should be included to accomodate this. Hence we will apply below model with Additive Error, Additive Trend and Additive Seasonality Model.

```{r ETS AAA, include=FALSE}
# Fit a model using ETS(A,A,A):
ETS_AAA_opt_dataset <- ets(dataset_train, model= "AAA")
# Check the AIC
ETS_AAA_opt_dataset
#Finding the coefficients
coef(ETS_AAA_opt_dataset)
#Forecasting the ANA model
Forecast_ETS_AAA_opt_dataset <- forecast(ETS_AAA_opt_dataset, h=h)$mean
plot(Forecast_ETS_AAA_opt_dataset)

# Error check for Forecast for ETS(A,A,A)
ETS_AAA_opt_dataset_errors <- dataset_test - Forecast_ETS_AAA_opt_dataset
ETS_AAA_ME_dataset<- mean(ETS_AAA_opt_dataset_errors) #Mean error
ETS_AAA_MSE_dataset<- mean(ETS_AAA_opt_dataset_errors^2) #Mean squared error
ETS_AAA_MAE_dataset<- mean(abs(ETS_AAA_opt_dataset_errors)) #Mean absolute error
ETS_AAA_RMSE_dataset<- sqrt(mean(ETS_AAA_opt_dataset_errors^2)) # Root mean squared error
ETS_AAA_MAPE_dataset<- 100 * mean(abs(ETS_AAA_opt_dataset_errors)/dataset_test)

## Rolling origin for ETS AAA

dataset_rolling_forecasts_ets_AAA <- matrix(NA, nrow=origins, ncol=H)
dataset_rolling_holdout_ETS_AAA <- matrix(NA, nrow=origins, ncol=H)
colnames(dataset_rolling_forecasts_ets_AAA) <- paste0("horizon",c(1:H))
rownames(dataset_rolling_forecasts_ets_AAA) <- paste0("origin",c(1:origins))
dimnames(dataset_rolling_holdout_ETS_AAA) <- dimnames(dataset_rolling_forecasts_ets_AAA)
for(i in 1:origins)
{
  # Create a ts object out of the dataset data
  dataset_rolling_train_set <- ts(dataset[1:(dataset_rolling_train_length+i-1)],
                                  frequency=frequency(dataset),
                                  start=start(dataset))
  
  # Write down the holdout values from the test set
  dataset_rolling_holdout_ETS_AAA[i,] <- dataset_rolling_test[i-1+(1:H)]
  
  # Produce forecasts and write them down
  dataset_rolling_forecasts_ets_AAA[i,] <- forecast(ets(dataset_rolling_train_set,"AAA"),h=H)$mean
}

## MAPE for Rolling origin of ETS AAA
Rolling_errors_dataset_ETS_AAA<- dataset_rolling_holdout_ETS_AAA - dataset_rolling_forecasts_ets_AAA
Rolling_ME_dataset_ETS_AAA<- mean(Rolling_errors_dataset_ETS_AAA) #Mean error
Rolling_MSE_dataset_ETS_AAA<- mean(Rolling_errors_dataset_ETS_AAA^2) #Mean squared error
Rolling_MAE_dataset_ETS_AAA<- mean(abs(Rolling_errors_dataset_ETS_AAA)) #Mean absolute error
Rolling_MAPE_dataset_ETS_AAA<- 100 * mean(abs(Rolling_errors_dataset_ETS_AAA)/dataset_rolling_holdout_ETS_AAA)
Rolling_RMSE_dataset_ETS_AAA<- sqrt(mean(Rolling_errors_dataset_ETS_AAA^2)) #Root mean squared error


```

#### Exponentail Smoothing Multiplicative Error, Additive Trend and Multiplicative Seasonality Model (M,A,M)

If we the see the ATM transactions are in increasing order in the dataset interms of their value, and we can spot this with the help of decompose graph as well. This suggest that we may need to consider the multiplicative models as well. As there is strong weekly seasonality, multiplicative seasonality with error could the possible choice of model here. Also I presume that, as their is strong presence of seasonality and its eveolving if you see 1997 and 1998 observations, multiplicative models with error and seasonality would be a good fit here and to accomodate this additive trend could be the possible choice of trend.

```{r ETS MAM, include=FALSE}
# Fit a model using ETS(M,A,M):
ETS_MAM_dataset <- ets(dataset_train, model = "MAM")
# Check the AIC
ETS_MAM_dataset
#Finding the coefficients
coef(ETS_MAM_dataset)
#Forecasting the ANA model
Forecast_ETS_MAM_dataset <- forecast(ETS_MAM_dataset, h=h)
plot(Forecast_ETS_MAM_dataset)

# Error check for Forecast for ETS(A,N,M)
ETS_MAM_dataset_errors <- dataset_test - (Forecast_ETS_MAM_dataset$mean)
ETS_MAM_ME_dataset<- mean(ETS_MAM_dataset_errors) #Mean error
ETS_MAM_MSE_dataset<- mean(ETS_MAM_dataset_errors^2) #Mean squared error
ETS_MAM_MAE_dataset<- mean(abs(ETS_MAM_dataset_errors)) #Mean absolute error
ETS_MAM_MAPE_dataset<- 100 * mean(abs(ETS_MAM_dataset_errors)/dataset_test) #Mean absolute percentage error
ETS_MAM_RMSE_dataset<- sqrt(mean(ETS_MAM_dataset_errors^2)) # Root mean squared error


## Rolling origin for ETS MAM

dataset_rolling_forecasts_ETS_MAM <- matrix(NA, nrow=origins, ncol=H)
dataset_rolling_holdout_ETS_MAM <- matrix(NA, nrow=origins, ncol=H)
colnames(dataset_rolling_forecasts_ETS_MAM) <- paste0("horizon",c(1:H))
rownames(dataset_rolling_forecasts_ETS_MAM) <- paste0("origin",c(1:origins))
dimnames(dataset_rolling_holdout_ETS_MAM) <- dimnames(dataset_rolling_forecasts_ETS_MAM)
for(i in 1:origins)
{
  # Create a ts object out of the dataset data
  dataset_rolling_train_set <- ts(dataset[1:(dataset_rolling_train_length+i-1)],
                                  frequency=frequency(dataset),
                                  start=start(dataset))
  
  # Write down the holdout values from the test set
  dataset_rolling_holdout_ETS_MAM[i,] <- dataset_rolling_test[i-1+(1:H)]
  
  # Produce forecasts and write them down
  dataset_rolling_forecasts_ETS_MAM[i,] <- forecast(ets(dataset_rolling_train_set,"MAM"),h=H)$mean
}

## MAPE for Rolling origin of ETS MAM
Rolling_errors_dataset_ETS_MAM<- dataset_rolling_holdout_ETS_MAM - dataset_rolling_forecasts_ETS_MAM
Rolling_ME_dataset_ETS_MAM<- mean(Rolling_errors_dataset_ETS_MAM) #Mean error
Rolling_MSE_dataset_ETS_MAM<- mean(Rolling_errors_dataset_ETS_MAM^2) #Mean squared error
Rolling_MAE_dataset_ETS_MAM<- mean(abs(Rolling_errors_dataset_ETS_MAM)) #Mean absolute error
Rolling_MAPE_dataset_ETS_MAM<- 100 * mean(abs(Rolling_errors_dataset_ETS_MAM)/dataset_rolling_holdout_ETS_MAM)
Rolling_RMSE_dataset_ETS_MAM<- sqrt(mean(Rolling_errors_dataset_ETS_MAM^2)) #Root mean squared error


```

#### Exponentail Smoothing Multiplicative Error, No Trend and Additive Seasonality Model (M,N,A)

As mentioned in above comment, multiplicative could be the potential choice for error and as the dataset does not show the strong presence of trend,we have consider no trend and additive seasonality in this model.

```{r ETS MNA, include=FALSE}
# Fit a model using ETS(M,N,A):
ETS_MNA_dataset <- ets(dataset_train, model = "MNA")
# Check the AIC
ETS_MNA_dataset
#Finding the coefficients
coef(ETS_MNA_dataset)
#Forecasting the MNA model
Forecast_ETS_MNA_dataset <- forecast(ETS_MNA_dataset, h=h)
plot(Forecast_ETS_MNA_dataset)


# Error check for Forecast for ETS(M,N,A)
ETS_MNA_dataset_errors <- dataset_test - (Forecast_ETS_MNA_dataset$mean)
ETS_MNA_ME_dataset<- mean(ETS_MNA_dataset_errors) #Mean error
ETS_MNA_MSE_dataset<- mean(ETS_MNA_dataset_errors^2) #Mean squared error
ETS_MNA_MAE_dataset<- mean(abs(ETS_MNA_dataset_errors)) #Mean absolute error
ETS_MNA_RMSE_dataset<- sqrt(mean(ETS_MNA_dataset_errors^2)) # Root mean squared error
ETS_MNA_MAPE_dataset<- 100 * mean(abs(ETS_MNA_dataset_errors)/dataset_test) #Mean absolute percentage error

## Rolling origin for ETS MNA

dataset_rolling_forecasts_ETS_MNA <- matrix(NA, nrow=origins, ncol=H)
dataset_rolling_holdout_ETS_MNA <- matrix(NA, nrow=origins, ncol=H)
colnames(dataset_rolling_forecasts_ETS_MNA) <- paste0("horizon",c(1:H))
rownames(dataset_rolling_forecasts_ETS_MNA) <- paste0("origin",c(1:origins))
dimnames(dataset_rolling_holdout_ETS_MNA) <- dimnames(dataset_rolling_forecasts_ETS_MNA)
for(i in 1:origins)
{
  # Create a ts object out of the dataset data
  dataset_rolling_train_set <- ts(dataset[1:(dataset_rolling_train_length+i-1)],
                                  frequency=frequency(dataset),
                                  start=start(dataset))
  
  # Write down the holdout values from the test set
  dataset_rolling_holdout_ETS_MNA[i,] <- dataset_rolling_test[i-1+(1:H)]
  
  # Produce forecasts and write them down
  dataset_rolling_forecasts_ETS_MNA[i,] <- forecast(ets(dataset_rolling_train_set,"MNA"),h=H)$mean
}

## MAPE for Rolling origin of ETS MNA
Rolling_errors_dataset_ETS_MNA<- dataset_rolling_holdout_ETS_MNA - dataset_rolling_forecasts_ETS_MNA
Rolling_ME_dataset_ETS_MNA<- mean(Rolling_errors_dataset_ETS_MNA) #Mean error
Rolling_MSE_dataset_ETS_MNA<- mean(Rolling_errors_dataset_ETS_MNA^2) #Mean squared error
Rolling_MAE_dataset_ETS_MNA<- mean(abs(Rolling_errors_dataset_ETS_MNA)) #Mean absolute error
Rolling_MAPE_dataset_ETS_MNA<- 100 * mean(abs(Rolling_errors_dataset_ETS_MNA)/dataset_rolling_holdout_ETS_MNA)
Rolling_RMSE_dataset_ETS_MNA<- sqrt(mean(Rolling_errors_dataset_ETS_MNA^2)) #Root mean squared error

```

#### Optimized method using ETS Function

If we run the dataset through the ETS ZZZ model, the ETS function suggested the ETS Multiplicative Error, No Trend and Multiplicative Seasonality (M,N,M) model as a right candidate for the forecasting. I could not argued against this as a best candidate because this model has lowest AIC, BIC, Mean absolute percentage error with static origin, Root Mean Square Error with static origin, Mean absolute percentage error with rolling origin and Root Mean Square Error with rolling origin. Please refer the table number - 

```{r Exponentail Smoothing ZZZ, include=FALSE}

# Calculate an Optimized ES Method using ETS()
ETS_optimised_dataset <- ets(dataset_train, model = "ZZZ")
# Check the AIC
ETS_optimised_dataset
# Coefficient of ETS optimized method
coef(ETS_optimised_dataset)

#Forecasting the ETS optimized model
Forecast_ETS_optimised_dataset <- forecast(ETS_optimised_dataset, h=h)
plot(Forecast_ETS_optimised_dataset)

# Error check for Forecast for ETS optimized
ETS_optimised_dataset_errors <- dataset_test - (Forecast_ETS_optimised_dataset$mean)
ETS_optimised_ME_dataset<- mean(ETS_optimised_dataset_errors) #Mean error
ETS_optimised_MSE_dataset<- mean(ETS_optimised_dataset_errors^2) #Mean squared error
ETS_optimised_MAE_dataset<- mean(abs(ETS_optimised_dataset_errors)) #Mean absolute error
ETS_optimised_MAPE_dataset<- 100 * mean(abs(ETS_optimised_dataset_errors)/dataset_test) #Mean absolute percentage error
ETS_optimised_RMSE_dataset<- sqrt(mean(ETS_optimised_dataset_errors^2)) # Root mean squared error


## Rolling origin for ETS optimized

dataset_rolling_forecasts_ETS_optimised<- matrix(NA, nrow=origins, ncol=H)
dataset_rolling_holdout_ETS_optimised <- matrix(NA, nrow=origins, ncol=H)
colnames(dataset_rolling_forecasts_ETS_optimised) <- paste0("horizon",c(1:H))
rownames(dataset_rolling_forecasts_ETS_optimised) <- paste0("origin",c(1:origins))
dimnames(dataset_rolling_holdout_ETS_optimised) <- dimnames(dataset_rolling_forecasts_ETS_optimised)
for(i in 1:origins)
{
  # Create a ts object out of the dataset data
  dataset_rolling_train_set <- ts(dataset[1:(dataset_rolling_train_length+i-1)],
                                  frequency=frequency(dataset),
                                  start=start(dataset))
  
  # Write down the holdout values from the test set
  dataset_rolling_holdout_ETS_optimised[i,] <- dataset_rolling_test[i-1+(1:H)]
  
  # Produce forecasts and write them down
  dataset_rolling_forecasts_ETS_optimised[i,] <- forecast(ets(dataset_rolling_train_set,"ZZZ"),h=H)$mean
}

## MAPE for Rolling origin of ETS optimized
Rolling_errors_dataset_ETS_optimised<- dataset_rolling_holdout_ETS_optimised - dataset_rolling_forecasts_ETS_optimised
Rolling_ME_dataset_ETS_optimised<- mean(Rolling_errors_dataset_ETS_optimised) #Mean error
Rolling_MSE_dataset_ETS_optimised<- mean(Rolling_errors_dataset_ETS_optimised^2) #Mean squared error
Rolling_MAE_dataset_ETS_optimised<- mean(abs(Rolling_errors_dataset_ETS_optimised)) #Mean absolute error
Rolling_MAPE_dataset_ETS_optimised<- 100 * mean(abs(Rolling_errors_dataset_ETS_optimised)/dataset_rolling_holdout_ETS_optimised)
Rolling_RMSE_dataset_ETS_optimised<- sqrt(mean(Rolling_errors_dataset_ETS_optimised^2)) #Root mean squared error

```

```{r Plot Exponentail Smoothing ZZZ, echo=FALSE}
plot(Forecast_ETS_optimised_dataset)
```

####  Recommended Model using the exponential smoothing -

As per the below summary table, ETS Optimized model (Multiplicative Error, No Trend and Multiplicative Seasonality (M,N,M) model) has lowest AIC value 7021.421 and lowest BIC value 7067.227 if compare with other models. Also, if we compare the  Mean absolute percentage error with static origin, Root Mean Square Error with static origin, Mean absolute percentage error with rolling origin and Root Mean Square Error with rolling origin values with other models, these values are very similar to other models. Based on this result the optimized model which is suggested by ETS function "R" is the good fit for forecasting this dataset.
The next best candidate for the model would be ETS (Multiplicative Error, No Trend and Additive Seasonality (M,N,A) model) based on the fact that AIC value is 2nd lowest 7029.001 with BIC as 7074.807. Also if compare the error measures MAPE is 2nd lowest along with RMSE. This model has done better in rolling origin error measues by being the lowest in the table values. Not only these values but as this dataset has strong seasonality with no trend, with possible multi seasonality factor, it makes sense to use this model.


### ARIMA/SARIMA models -

For ARIMA models to be fitted, we need to make sure time series is stationary. Our time series is not stationary as it contains weekly seasonality. But when we did KPSS/ADF test, test results were inconclusive. But as we know it contains seasonality hence we can reply on visual inspection that time series is not stationary.
In order to make it stationary, we will try to first order differecing and below is the plot of residuals-

```{r ARIMA Prep, include=FALSE}

# Perform KPSS and ADF test on time series
kpss.test(dataset_train)
adf.test(dataset_train)

# Plot the ACF and PACF plots
tsdisplay(dataset)

#Difference order
nsdiffs(dataset)  # Seasonal diff
ndiffs(dataset) # Normal diff

# First order Difference
diff_dataset_train <- diff(dataset_train)

# Plot ACF and PACF of first differences
tsdisplay(diff_dataset_train)

# KPSS/ADF test of 1st order diff series
kpss.test(diff_dataset_train)
adf.test(diff_dataset_train)

# Plot ACF and PACF of first and seasonal differences
tsdisplay(diff(diff((dataset_train),lag=7)))

# Plot ACF and PACF of seasonal differences
tsdisplay(diff(dataset_train, lag=7))

# Second order diff
diff2_data <- diff(dataset_train, differences=2)
tsdisplay(diff2_data)

## Fitting the ARIMA/SARIMA Models
tsdisplay(dataset_train)

```

```{r KPSS/ADF of 1st order diff, echo=FALSE}
# First order Difference
diff_dataset_train <- diff(dataset_train)

# Plot ACF and PACF of first differences
tsdisplay(diff_dataset_train)
```

We can clearly see after first order differecing, time series is not become stationary.but when I did the KPSS/ADF test on 1st order difference time series, seems like results are saying it has become stationary.

```{r KPSS/ADF test of 1st order diff series ,include=FALSE}
# KPSS/ADF test of 1st order diff series
kpss.test(diff_dataset_train)
adf.test(diff_dataset_train)
```

Now as after the 1st differcencing it seems like the data has become stationary, we will proceed with identification of ARIMA model. To do that, we will analyse the ACF and PACF plot as below and will establish a appropriate model.

```{r Plot the ACF and PACF plots, echo=FALSE}
# Plot the ACF and PACF plots
tsdisplay(dataset)
```

As the PACF cuts off after lag 7 that means Auto regressive (AR) and ACF does not die away hence ARIMA Model â€“ Non-Stationary would be a good suit also that means we need differencing.

In general, PACF cuts off after the value p that means AR value, in our case its lag 7 and in general, ACF cuts off after the value q that means MA value, in our case its not cutting off but exponentially decaying. Seasonal AR models are preferred in this case.

When tried ndiffs test on our data set, it gives back that we need 1 order of non seasonal differencing.
When we specify test to be used for ndiffs as KPSS it is giving back as 1 order differecing is needed but when we tried ADF or PP test then its returning back as zero.

When tried nsdiffs test on our data set, it gives back that we need zero order of non seasonal differencing.
Spike at Lag in ACF strongly suggest non seasonal MA 1 and the significant spike at lag 7 suggests a seasonal MA 1.

Below models I think could be potential choice for good fit in forecasting based on the above discussion. While finding the potential models I consider the Ljung- Box test to evaluate whether the residual lags have any co-correlation between them. Also, I checked whether the residuals are under the significance level of 5 % in ACF/PACF plot which essentially means that the residuals have no or in insignificant correlation between them and there is insignificant presence of any pattern.

If we compare the AIC, BIC values along with the error measures against test dataset, ARIMA model is the recommended model among st all this.

If we see the residuals for the ARIMA(7,1,1)(0,1,3)[7],ARIMA(2,0,2)(0,1,2)[7],ARIMA(2,0,2)(1,1,1)[7] and ARIMA(1,1,3)(2,1,2)[7]  lags can be seen outside the significance level which means lags could be related to the previous ones but this 4 models does have only one lag outside the significance level which can be ignore to some extent if everything falls under.
The model ARIMA(7,1,1)(0,1,3)[7] has AR value as 7 which is significantly large and could not be the right choice to proceed as the parameters are unnecessarily increased. Hence we will not select ARIMA(7,1,1)(0,1,3)[7]. ARIMA(1,1,3)(2,1,2)[7] for this model the AIC, BIC values are greater if we compare with others with residual lag outside the significance level hence we will not proceed with this model.Out of remaining 4 models, ARIMA(3,0,2)(1,1,2)[7] and ARIMA(4,0,2)(3,1,1)[7] model does not have any residual lag outside the significance level which is good sign to proceed but the model ARIMA(4,0,2)(3,1,1)[7] has greater AIC, BIC, MAPE, MAPE with rolling origin, RMSE, RMSE with rolling origin than other 3 hence we will not proceed with this.
Out of the remaining 3, ARIMA(3,0,2)(1,1,2)[7] has lowest AIC along with MAPE and has no residuals lags outside the significance level hence this would be the good candidiate for forecasting.

```{r ARIMA Models, include=FALSE}
# Seasonal ARIMA

Arima711_Seasoanl013_dataset<- Arima(dataset_train, order=c(7,1,1),  seasonal=c(0,1,3))
Arima202_Seasoanl012_dataset<- Arima(dataset_train, order=c(2,0,2),  seasonal=c(0,1,2))
Arima202_Seasoanl111_dataset<- Arima(dataset_train, order=c(2,0,2),  seasonal=c(1,1,1))
Arima113_Seasoanl212_dataset<- Arima(dataset_train, order=c(1,1,3),  seasonal=c(2,1,2))
#Arima212_Seasoanl102_dataset<- Arima(dataset_train, order=c(2,1,2),  seasonal=c(1,0,2))
Arima302_Seasoanl112_dataset<- Arima(dataset_train, order=c(3,0,2),  seasonal=c(1,1,2))
Arima402_Seasoanl311_dataset<- Arima(dataset_train, order=c(4,0,2),  seasonal=c(3,1,1))

  
# Check the coeff of SARIMA Models

Arima711_Seasoanl013_dataset # 2978.06   4384.2   
Arima202_Seasoanl012_dataset # 2946.29   4385.27   
Arima202_Seasoanl111_dataset # 2945.85   4385.02   
Arima113_Seasoanl212_dataset # 2953.3    4395.27   
#Arima212_Seasoanl102_dataset # 2986.39   4420.83   
Arima302_Seasoanl112_dataset # 2947.62   4382.65   
Arima402_Seasoanl311_dataset

# Check residuals of SARIMA model

checkresiduals(Arima711_Seasoanl013_dataset) 
checkresiduals(Arima202_Seasoanl012_dataset)
checkresiduals(Arima202_Seasoanl111_dataset)
checkresiduals(Arima113_Seasoanl212_dataset)
#checkresiduals(Arima212_Seasoanl102_dataset)
checkresiduals(Arima302_Seasoanl112_dataset)
checkresiduals(Arima402_Seasoanl311_dataset)

# Check ACF/PACF plot of residuals

tsdisplay(residuals(Arima711_Seasoanl013_dataset))
tsdisplay(residuals(Arima202_Seasoanl012_dataset)) # Failed in Residuals plot
tsdisplay(residuals(Arima202_Seasoanl111_dataset)) # Failed in Residuals plot
tsdisplay(residuals(Arima113_Seasoanl212_dataset))
#tsdisplay(residuals(Arima212_Seasoanl102_dataset)) # Failed in Residuals plot/full
tsdisplay(residuals(Arima302_Seasoanl112_dataset)) # Failed in Residuals plot
tsdisplay(residuals(Arima402_Seasoanl311_dataset))

# Apply all these ARIMA/SARIMA models for forecasting

# Seasonal ARIMA

FC_Arima711_Seasoanl013_dataset <- forecast(Arima711_Seasoanl013_dataset, h=h)$mean
FC_Arima202_Seasoanl012_dataset <- forecast(Arima202_Seasoanl012_dataset, h=h)$mean
FC_Arima202_Seasoanl111_dataset <- forecast(Arima202_Seasoanl111_dataset, h=h)$mean
FC_Arima113_Seasoanl212_dataset <- forecast(Arima113_Seasoanl212_dataset, h=h)$mean
#FC_Arima212_Seasoanl102_dataset <- forecast(Arima212_Seasoanl102_dataset, h=h)$mean
FC_Arima302_Seasoanl112_dataset <- forecast(Arima302_Seasoanl112_dataset, h=h)$mean
FC_Arima402_Seasoanl311_dataset <- forecast(Arima402_Seasoanl311_dataset, h=h)$mean


```

```{r Arima 711_Seasoanl 013, include=FALSE}

# Model implementation
Arima711_Seasoanl013_dataset<- Arima(dataset_train, order=c(7,1,1),  seasonal=c(0,1,3))

# Check the coeff of SARIMA Models
Arima711_Seasoanl013_dataset # 2978.06   4384.2   

# Check residuals of SARIMA model
checkresiduals(Arima711_Seasoanl013_dataset) 

# Check ACF/PACF plot of residuals
tsdisplay(residuals(Arima711_Seasoanl013_dataset))

# Forecasting
FC_Arima711_Seasoanl013_dataset <- forecast(Arima711_Seasoanl013_dataset, h=h)$mean

# Error measures
Arima711_Seasoanl013_dataset_errors <- dataset_test - FC_Arima711_Seasoanl013_dataset
Arima711_Seasoanl013_dataset_ME <- mean(Arima711_Seasoanl013_dataset_errors) #Mean error
Arima711_Seasoanl013_dataset_MSE <- mean(Arima711_Seasoanl013_dataset_errors^2) #Mean squared error
Arima711_Seasoanl013_dataset_MAE <- mean(abs(Arima711_Seasoanl013_dataset_errors)) #Mean absolute error
Arima711_Seasoanl013_dataset_MAPE <- 100 * mean(abs(Arima711_Seasoanl013_dataset_errors)/dataset_test)
Arima711_Seasoanl013_dataset_RMSE <- sqrt(mean(Arima711_Seasoanl013_dataset_errors^2)) # Root mean squared error

## Rolling origin for Arima711_Seasoanl013

dataset_rolling_forecasts_Arima711_Seasoanl013 <- matrix(NA, nrow=origins, ncol=H)
dataset_rolling_holdout_Arima711_Seasoanl013 <- matrix(NA, nrow=origins, ncol=H)
colnames(dataset_rolling_forecasts_Arima711_Seasoanl013) <- paste0("horizon",c(1:H))
rownames(dataset_rolling_forecasts_Arima711_Seasoanl013) <- paste0("origin",c(1:origins))
dimnames(dataset_rolling_holdout_Arima711_Seasoanl013) <- dimnames(dataset_rolling_forecasts_Arima711_Seasoanl013)
for(i in 1:origins)
{
  # Create a ts object out of the dataset data
  dataset_rolling_train_set <- ts(dataset[1:(dataset_rolling_train_length+i-1)],
                                  frequency=frequency(dataset),
                                  start=start(dataset))
  
  # Write down the holdout values from the test set
  dataset_rolling_holdout_Arima711_Seasoanl013[i,] <- dataset_rolling_test[i-1+(1:H)]
  
  # Produce forecasts and write them down
  dataset_rolling_forecasts_Arima711_Seasoanl013[i,] <- forecast(arima(dataset_rolling_train_set, order=c(7,1,1),  seasonal=c(0,1,3)),h=H)$mean
}


## MAPE for Rolling origin of Arima711_Seasoanl013
Rolling_errors_dataset_Arima711_Seasoanl013<- dataset_rolling_holdout_Arima711_Seasoanl013 - dataset_rolling_forecasts_Arima711_Seasoanl013
Rolling_ME_dataset_Arima711_Seasoanl013<- mean(Rolling_errors_dataset_Arima711_Seasoanl013) #Mean error
Rolling_MSE_dataset_Arima711_Seasoanl013<- mean(Rolling_errors_dataset_Arima711_Seasoanl013^2) #Mean squared error
Rolling_MAE_dataset_Arima711_Seasoanl013<- mean(abs(Rolling_errors_dataset_Arima711_Seasoanl013)) #Mean absolute error
Rolling_MAPE_dataset_Arima711_Seasoanl013<- 100 * mean(abs(Rolling_errors_dataset_Arima711_Seasoanl013)/dataset_rolling_holdout_Arima711_Seasoanl013)
Rolling_RMSE_dataset_Arima711_Seasoanl013<- sqrt(mean(Rolling_errors_dataset_Arima711_Seasoanl013^2)) #Root mean squared error

```

```{r Plot Arima 711_Seasoanl 013, echo=FALSE}
tsdisplay(residuals(Arima711_Seasoanl013_dataset))
```

```{r Arima 202 Seasoanl 012, include=FALSE}
Arima202_Seasoanl012_dataset<- Arima(dataset_train, order=c(2,0,2),  seasonal=c(0,1,2))
Arima202_Seasoanl012_dataset # 2946.29   4385.27  

checkresiduals(Arima202_Seasoanl012_dataset)

tsdisplay(residuals(Arima202_Seasoanl012_dataset)) # Failed in Residuals plot

FC_Arima202_Seasoanl012_dataset <- forecast(Arima202_Seasoanl012_dataset, h=h)$mean

Arima202_Seasoanl012_dataset_errors <- dataset_test - FC_Arima202_Seasoanl012_dataset
Arima202_Seasoanl012_dataset_ME <- mean(Arima202_Seasoanl012_dataset_errors) #Mean error
Arima202_Seasoanl012_dataset_MSE <- mean(Arima202_Seasoanl012_dataset_errors^2) #Mean squared error
Arima202_Seasoanl012_dataset_MAE <- mean(abs(Arima202_Seasoanl012_dataset_errors)) #Mean absolute error
Arima202_Seasoanl012_dataset_MAPE <- 100 * mean(abs(Arima202_Seasoanl012_dataset_errors)/dataset_test)
Arima202_Seasoanl012_dataset_RMSE <- sqrt(mean(Arima202_Seasoanl012_dataset_errors^2)) # Root mean squared error

## Rolling origin for Arima202_Seasoanl012

dataset_rolling_forecasts_Arima202_Seasoanl012 <- matrix(NA, nrow=origins, ncol=H)
dataset_rolling_holdout_Arima202_Seasoanl012 <- matrix(NA, nrow=origins, ncol=H)
colnames(dataset_rolling_forecasts_Arima202_Seasoanl012) <- paste0("horizon",c(1:H))
rownames(dataset_rolling_forecasts_Arima202_Seasoanl012) <- paste0("origin",c(1:origins))
dimnames(dataset_rolling_holdout_Arima202_Seasoanl012) <- dimnames(dataset_rolling_forecasts_Arima202_Seasoanl012)
for(i in 1:origins)
{
  # Create a ts object out of the dataset data
  dataset_rolling_train_set <- ts(dataset[1:(dataset_rolling_train_length+i-1)],
                                  frequency=frequency(dataset),
                                  start=start(dataset))
  
  # Write down the holdout values from the test set
  dataset_rolling_holdout_Arima202_Seasoanl012[i,] <- dataset_rolling_test[i-1+(1:H)]
  
  # Produce forecasts and write them down
  dataset_rolling_forecasts_Arima202_Seasoanl012[i,] <- forecast(arima(dataset_rolling_train_set, order=c(2,0,2),  seasonal=c(0,1,2)),h=H)$mean
}

## MAPE for Rolling origin of Arima202_Seasoanl012
Rolling_errors_dataset_Arima202_Seasoanl012<- dataset_rolling_holdout_Arima202_Seasoanl012 - dataset_rolling_forecasts_Arima202_Seasoanl012
Rolling_ME_dataset_Arima202_Seasoanl012<- mean(Rolling_errors_dataset_Arima202_Seasoanl012) #Mean error
Rolling_MSE_dataset_Arima202_Seasoanl012<- mean(Rolling_errors_dataset_Arima202_Seasoanl012^2) #Mean squared error
Rolling_MAE_dataset_Arima202_Seasoanl012<- mean(abs(Rolling_errors_dataset_Arima202_Seasoanl012)) #Mean absolute error
Rolling_MAPE_dataset_Arima202_Seasoanl012<- 100 * mean(abs(Rolling_errors_dataset_Arima202_Seasoanl012)/dataset_rolling_holdout_Arima202_Seasoanl012)
Rolling_RMSE_dataset_Arima202_Seasoanl012<- sqrt(mean(Rolling_errors_dataset_Arima202_Seasoanl012^2)) #Root mean squared error



```

```{r Plot Arima 202 Seasoanl 012, echo=FALSE}
tsdisplay(residuals(Arima202_Seasoanl012_dataset))
```

```{r Arima 202 Seasoanl 111, eval=FALSE, include=FALSE}
Arima202_Seasoanl111_dataset<- Arima(dataset_train, order=c(2,0,2),  seasonal=c(1,1,1))

checkresiduals(Arima202_Seasoanl111_dataset)

tsdisplay(residuals(Arima202_Seasoanl111_dataset)) # Failed in Residuals plot

FC_Arima202_Seasoanl111_dataset <- forecast(Arima202_Seasoanl111_dataset, h=h)$mean

Arima202_Seasoanl111_dataset_errors <- dataset_test - FC_Arima202_Seasoanl111_dataset
Arima202_Seasoanl111_dataset_ME <- mean(Arima202_Seasoanl111_dataset_errors) #Mean error
Arima202_Seasoanl111_dataset_MSE <- mean(Arima202_Seasoanl111_dataset_errors^2) #Mean squared error
Arima202_Seasoanl111_dataset_MAE <- mean(abs(Arima202_Seasoanl111_dataset_errors)) #Mean absolute error
Arima202_Seasoanl111_dataset_MAPE <- 100 * mean(abs(Arima202_Seasoanl111_dataset_errors)/dataset_test)
Arima202_Seasoanl111_dataset_RMSE <- sqrt(mean(Arima202_Seasoanl111_dataset_errors^2)) # Root mean squared error

## Rolling origin for Arima202_Seasoanl111

dataset_rolling_forecasts_Arima202_Seasoanl111 <- matrix(NA, nrow=origins, ncol=H)
dataset_rolling_holdout_Arima202_Seasoanl111 <- matrix(NA, nrow=origins, ncol=H)
colnames(dataset_rolling_forecasts_Arima202_Seasoanl111) <- paste0("horizon",c(1:H))
rownames(dataset_rolling_forecasts_Arima202_Seasoanl111) <- paste0("origin",c(1:origins))
dimnames(dataset_rolling_holdout_Arima202_Seasoanl111) <- dimnames(dataset_rolling_forecasts_Arima202_Seasoanl111)
for(i in 1:origins)
{
  # Create a ts object out of the dataset data
  dataset_rolling_train_set <- ts(dataset[1:(dataset_rolling_train_length+i-1)],
                                  frequency=frequency(dataset),
                                  start=start(dataset))
  
  # Write down the holdout values from the test set
  dataset_rolling_holdout_Arima202_Seasoanl111[i,] <- dataset_rolling_test[i-1+(1:H)]
  
  # Produce forecasts and write them down
  dataset_rolling_forecasts_Arima202_Seasoanl111[i,] <- forecast(arima(dataset_rolling_train_set, order=c(2,0,2),  seasonal=c(1,1,1)),h=H)$mean
}

## MAPE for Rolling origin of Arima202_Seasoanl111
Rolling_errors_dataset_Arima202_Seasoanl111<- dataset_rolling_holdout_Arima202_Seasoanl111 - dataset_rolling_forecasts_Arima202_Seasoanl111
Rolling_ME_dataset_Arima202_Seasoanl111<- mean(Rolling_errors_dataset_Arima202_Seasoanl111) #Mean error
Rolling_MSE_dataset_Arima202_Seasoanl111<- mean(Rolling_errors_dataset_Arima202_Seasoanl111^2) #Mean squared error
Rolling_MAE_dataset_Arima202_Seasoanl111<- mean(abs(Rolling_errors_dataset_Arima202_Seasoanl111)) #Mean absolute error
Rolling_MAPE_dataset_Arima202_Seasoanl111<- 100 * mean(abs(Rolling_errors_dataset_Arima202_Seasoanl111)/dataset_rolling_holdout_Arima202_Seasoanl111)
Rolling_RMSE_dataset_Arima202_Seasoanl111<- sqrt(mean(Rolling_errors_dataset_Arima202_Seasoanl111^2)) #Root mean squared error



```

```{r Plot Arima 202 Seasoanl 111, echo=FALSE}
tsdisplay(residuals(Arima202_Seasoanl111_dataset))
```

```{r Arima 113 Seasoanl 212, include=FALSE}
Arima113_Seasoanl212_dataset<- Arima(dataset_train, order=c(1,1,3),  seasonal=c(2,1,2))

checkresiduals(Arima113_Seasoanl212_dataset)

tsdisplay(residuals(Arima113_Seasoanl212_dataset))

FC_Arima113_Seasoanl212_dataset <- forecast(Arima113_Seasoanl212_dataset, h=h)$mean


Arima113_Seasoanl212_dataset_errors <- dataset_test - FC_Arima113_Seasoanl212_dataset
Arima113_Seasoanl212_dataset_ME <- mean(Arima113_Seasoanl212_dataset_errors) #Mean error
Arima113_Seasoanl212_dataset_MSE <- mean(Arima113_Seasoanl212_dataset_errors^2) #Mean squared error
Arima113_Seasoanl212_dataset_MAE <- mean(abs(Arima113_Seasoanl212_dataset_errors)) #Mean absolute error
Arima113_Seasoanl212_dataset_MAPE <- 100 * mean(abs(Arima113_Seasoanl212_dataset_errors)/dataset_test)
Arima113_Seasoanl212_dataset_RMSE <- sqrt(mean(Arima113_Seasoanl212_dataset_errors^2)) # Root mean squared error

## Rolling origin for Arima113_Seasoanl212

dataset_rolling_forecasts_Arima113_Seasoanl212 <- matrix(NA, nrow=origins, ncol=H)
dataset_rolling_holdout_Arima113_Seasoanl212 <- matrix(NA, nrow=origins, ncol=H)
colnames(dataset_rolling_forecasts_Arima113_Seasoanl212) <- paste0("horizon",c(1:H))
rownames(dataset_rolling_forecasts_Arima113_Seasoanl212) <- paste0("origin",c(1:origins))
dimnames(dataset_rolling_holdout_Arima113_Seasoanl212) <- dimnames(dataset_rolling_forecasts_Arima113_Seasoanl212)
for(i in 1:origins)
{
  # Create a ts object out of the dataset data
  dataset_rolling_train_set <- ts(dataset[1:(dataset_rolling_train_length+i-1)],
                                  frequency=frequency(dataset),
                                  start=start(dataset))
  
  # Write down the holdout values from the test set
  dataset_rolling_holdout_Arima113_Seasoanl212[i,] <- dataset_rolling_test[i-1+(1:H)]
  
  # Produce forecasts and write them down
  dataset_rolling_forecasts_Arima113_Seasoanl212[i,] <- forecast(arima(dataset_rolling_train_set, order=c(1,1,3),  seasonal=c(2,1,2)),h=H)$mean
  
}

## MAPE for Rolling origin of Arima113_Seasoanl212
Rolling_errors_dataset_Arima113_Seasoanl212<- dataset_rolling_holdout_Arima113_Seasoanl212 - dataset_rolling_forecasts_Arima113_Seasoanl212
Rolling_ME_dataset_Arima113_Seasoanl212<- mean(Rolling_errors_dataset_Arima113_Seasoanl212) #Mean error
Rolling_MSE_dataset_Arima113_Seasoanl212<- mean(Rolling_errors_dataset_Arima113_Seasoanl212^2) #Mean squared error
Rolling_MAE_dataset_Arima113_Seasoanl212<- mean(abs(Rolling_errors_dataset_Arima113_Seasoanl212)) #Mean absolute error
Rolling_MAPE_dataset_Arima113_Seasoanl212<- 100 * mean(abs(Rolling_errors_dataset_Arima113_Seasoanl212)/dataset_rolling_holdout_Arima113_Seasoanl212)
Rolling_RMSE_dataset_Arima113_Seasoanl212<- sqrt(mean(Rolling_errors_dataset_Arima113_Seasoanl212^2)) #Root mean squared error

```

```{r Plot Arima 113 Seasoanl 212, echo=FALSE}
tsdisplay(residuals(Arima113_Seasoanl212_dataset))
```

```{r Arima 302 Seasoanl 112, include=FALSE}
Arima302_Seasoanl112_dataset<- Arima(dataset_train, order=c(3,0,2),  seasonal=c(1,1,2))

checkresiduals(Arima302_Seasoanl112_dataset)

tsdisplay(residuals(Arima302_Seasoanl112_dataset)) # Failed in Residuals plot

FC_Arima302_Seasoanl112_dataset <- forecast(Arima302_Seasoanl112_dataset, h=h)$mean


Arima302_Seasoanl112_dataset_errors <- dataset_test - FC_Arima302_Seasoanl112_dataset
Arima302_Seasoanl112_dataset_ME <- mean(Arima302_Seasoanl112_dataset_errors) #Mean error
Arima302_Seasoanl112_dataset_MSE <- mean(Arima302_Seasoanl112_dataset_errors^2) #Mean squared error
Arima302_Seasoanl112_dataset_MAE <- mean(abs(Arima302_Seasoanl112_dataset_errors)) #Mean absolute error
Arima302_Seasoanl112_dataset_MAPE <- 100 * mean(abs(Arima302_Seasoanl112_dataset_errors)/dataset_test)
Arima302_Seasoanl112_dataset_RMSE <- sqrt(mean(Arima302_Seasoanl112_dataset_errors^2)) # Root mean squared error

## Rolling origin for Arima302_Seasoanl112

dataset_rolling_forecasts_Arima302_Seasoanl112 <- matrix(NA, nrow=origins, ncol=H)
dataset_rolling_holdout_Arima302_Seasoanl112 <- matrix(NA, nrow=origins, ncol=H)
colnames(dataset_rolling_forecasts_Arima302_Seasoanl112) <- paste0("horizon",c(1:H))
rownames(dataset_rolling_forecasts_Arima302_Seasoanl112) <- paste0("origin",c(1:origins))
dimnames(dataset_rolling_holdout_Arima302_Seasoanl112) <- dimnames(dataset_rolling_forecasts_Arima302_Seasoanl112)
for(i in 1:origins)
{
  # Create a ts object out of the dataset data
  dataset_rolling_train_set <- ts(dataset[1:(dataset_rolling_train_length+i-1)],
                                  frequency=frequency(dataset),
                                  start=start(dataset))
  
  # Write down the holdout values from the test set
  dataset_rolling_holdout_Arima302_Seasoanl112[i,] <- dataset_rolling_test[i-1+(1:H)]
  
  # Produce forecasts and write them down
  dataset_rolling_forecasts_Arima302_Seasoanl112[i,] <- forecast(arima(dataset_rolling_train_set, order=c(3,0,2),  seasonal=c(1,1,2)),h=H)$mean
}

## MAPE for Rolling origin of Arima302_Seasoanl112
Rolling_errors_dataset_Arima302_Seasoanl112<- dataset_rolling_holdout_Arima302_Seasoanl112 - dataset_rolling_forecasts_Arima302_Seasoanl112
Rolling_ME_dataset_Arima302_Seasoanl112<- mean(Rolling_errors_dataset_Arima302_Seasoanl112) #Mean error
Rolling_MSE_dataset_Arima302_Seasoanl112<- mean(Rolling_errors_dataset_Arima302_Seasoanl112^2) #Mean squared error
Rolling_MAE_dataset_Arima302_Seasoanl112<- mean(abs(Rolling_errors_dataset_Arima302_Seasoanl112)) #Mean absolute error
Rolling_MAPE_dataset_Arima302_Seasoanl112<- 100 * mean(abs(Rolling_errors_dataset_Arima302_Seasoanl112)/dataset_rolling_holdout_Arima302_Seasoanl112)
Rolling_RMSE_dataset_Arima302_Seasoanl112<- sqrt(mean(Rolling_errors_dataset_Arima302_Seasoanl112^2)) #Root mean squared error


```

```{r Plot Arima 302 Seasoanl 112, echo=FALSE}
tsdisplay(residuals(Arima302_Seasoanl112_dataset))
```

```{r Arima 402 Seasoanl311, eval=FALSE, include=FALSE}
Arima402_Seasoanl311_dataset<- Arima(dataset_train,order=c(4,0,2),seasonal=c(3,1,1))

checkresiduals(Arima402_Seasoanl311_dataset)

tsdisplay(residuals(Arima402_Seasoanl311_dataset))


FC_Arima402_Seasoanl311_dataset <- forecast(Arima402_Seasoanl311_dataset, h=h)$mean


Arima402_Seasoanl311_dataset_errors <- dataset_test - FC_Arima402_Seasoanl311_dataset
Arima402_Seasoanl311_dataset_ME <- mean(Arima402_Seasoanl311_dataset_errors) #Mean error
Arima402_Seasoanl311_dataset_MSE <- mean(Arima402_Seasoanl311_dataset_errors^2) #Mean squared error
Arima402_Seasoanl311_dataset_MAE <- mean(abs(Arima402_Seasoanl311_dataset_errors)) #Mean absolute error
Arima402_Seasoanl311_dataset_MAPE <- 100 * mean(abs(Arima402_Seasoanl311_dataset_errors)/dataset_test)
Arima402_Seasoanl311_dataset_RMSE <- sqrt(mean(Arima402_Seasoanl311_dataset_errors^2)) # Root mean squared error


## Rolling origin for Arima402_Seasoanl311

dataset_rolling_forecasts_Arima402_Seasoanl311 <- matrix(NA, nrow=origins, ncol=H)
dataset_rolling_holdout_Arima402_Seasoanl311 <- matrix(NA, nrow=origins, ncol=H)
colnames(dataset_rolling_forecasts_Arima402_Seasoanl311) <- paste0("horizon",c(1:H))
rownames(dataset_rolling_forecasts_Arima402_Seasoanl311) <- paste0("origin",c(1:origins))
dimnames(dataset_rolling_holdout_Arima402_Seasoanl311) <- dimnames(dataset_rolling_forecasts_Arima402_Seasoanl311)
for(i in 1:origins)
{
  # Create a ts object out of the dataset data
  dataset_rolling_train_set <- ts(dataset[1:(dataset_rolling_train_length+i-1)],
                                  frequency=frequency(dataset),
                                  start=start(dataset))
  
  # Write down the holdout values from the test set
  dataset_rolling_holdout_Arima402_Seasoanl311[i,] <- dataset_rolling_test[i-1+(1:H)]
  
  # Produce forecasts and write them down
  dataset_rolling_forecasts_Arima402_Seasoanl311[i,] <- forecast(arima(dataset_rolling_train_set, order=c(4,0,2),  seasonal=c(3,1,1)),h=H)$mean
  
}

## MAPE for Rolling origin of Arima402_Seasoanl311
Rolling_errors_dataset_Arima402_Seasoanl311<- dataset_rolling_holdout_Arima402_Seasoanl311 - dataset_rolling_forecasts_Arima402_Seasoanl311
Rolling_ME_dataset_Arima402_Seasoanl311<- mean(Rolling_errors_dataset_Arima402_Seasoanl311) #Mean error
Rolling_MSE_dataset_Arima402_Seasoanl311<- mean(Rolling_errors_dataset_Arima402_Seasoanl311^2) #Mean squared error
Rolling_MAE_dataset_Arima402_Seasoanl311<- mean(abs(Rolling_errors_dataset_Arima402_Seasoanl311)) #Mean absolute error
Rolling_MAPE_dataset_Arima402_Seasoanl311<- 100 * mean(abs(Rolling_errors_dataset_Arima402_Seasoanl311)/dataset_rolling_holdout_Arima402_Seasoanl311)
Rolling_RMSE_dataset_Arima402_Seasoanl311<- sqrt(mean(Rolling_errors_dataset_Arima402_Seasoanl311^2)) #Root mean squared error


```

```{r Plot Arima 402 Seasoanl311, echo=FALSE}
tsdisplay(residuals(Arima402_Seasoanl311_dataset))
```


#### Optimized ARIMA Model

If we run the dataset on auto-arima function, it has recommended ARIMA(2,1,2)(0,0,2)[7] model. if we analyse the model it has AIC value as 4398.57, BIC value as 4430.63 and also fail's the Ljung-Box test of auto correlations of a time series which means that residuals could be related to previous lags. If we carefully the residuals plot then you would see there are numerous lags outside the significance level which is not a sign of stationary. Also, MAPE and RMSE are comparatively higher than the previous discussed model so we will not be proceeding further with automatically built model.

```{r Auto ARIMA, include=FALSE}

# Try Auto ARIMA Model
auto_fit_dataset <- auto.arima(dataset_train,  trace = TRUE)
auto_fit_dataset
Forecast_auto_fit_dataset <- forecast(auto_fit_dataset, h = h)$mean
#Error measures for auto arima
auto_errors_dataset = dataset_test - Forecast_auto_fit_dataset
auto_ME_dataset <- mean(auto_errors_dataset) #Mean error
auto_MSE_dataset <- mean(auto_errors_dataset^2) #Mean squared error
auto_MAE_dataset <- mean(abs(auto_errors_dataset)) #Mean absolute error
auto_MAPE_dataset <- 100 * mean(abs(auto_errors_dataset)/dataset_test) #Mean absolute percentage error
auto_RMSE_dataset <- sqrt(mean(auto_errors_dataset^2)) #Root mean squared error

## Rolling origin for Auto Arima

dataset_rolling_forecasts_AutoArima <- matrix(NA, nrow=origins, ncol=H)
dataset_rolling_holdout_AutoArima <- matrix(NA, nrow=origins, ncol=H)
colnames(dataset_rolling_forecasts_AutoArima) <- paste0("horizon",c(1:H))
rownames(dataset_rolling_forecasts_AutoArima) <- paste0("origin",c(1:origins))
dimnames(dataset_rolling_holdout_AutoArima) <- dimnames(dataset_rolling_forecasts_AutoArima)
for(i in 1:origins)
{
  # Create a ts object out of the dataset data
  dataset_rolling_train_set <- ts(dataset[1:(dataset_rolling_train_length+i-1)],
                                  frequency=frequency(dataset),
                                  start=start(dataset))
  
  # Write down the holdout values from the test set
  #dataset_rolling_holdout_ES_ANA[i,] <- dataset_rolling_test[i-1+(1:h)]
  dataset_rolling_holdout_AutoArima[i,] <- dataset_rolling_test[i-1+(1:H)]
  
  # Produce forecasts and write them down
  dataset_rolling_forecasts_AutoArima[i,] <- forecast(auto.arima(dataset_rolling_train_set),h=H)$mean
}

## MAPE for Rolling origin of Auto Arima
Rolling_errors_dataset_AutoArima<- dataset_rolling_holdout_AutoArima - dataset_rolling_forecasts_AutoArima
Rolling_ME_dataset_AutoArima<- mean(Rolling_errors_dataset_AutoArima) #Mean error
Rolling_MSE_dataset_AutoArima<- mean(Rolling_errors_dataset_AutoArima^2) #Mean squared error
Rolling_MAE_dataset_AutoArima<- mean(abs(Rolling_errors_dataset_AutoArima)) #Mean absolute error
Rolling_MAPE_dataset_AutoArima<- 100 * mean(abs(Rolling_errors_dataset_AutoArima)/dataset_rolling_holdout_AutoArima)
Rolling_RMSE_dataset_AutoArima<- sqrt(mean(Rolling_errors_dataset_AutoArima^2)) #Root mean squared error

```

```{r eval=FALSE, include=FALSE}
# Create summary table
Summary_stats<- rbind(Summary_stats,c(Arima113_Seasoanl212_dataset_ME,Arima113_Seasoanl212_dataset_MSE,Arima113_Seasoanl212_dataset_MAE,Arima113_Seasoanl212_dataset_MAPE,Arima113_Seasoanl212_dataset_RMSE))
Summary_stats<- rbind(Summary_stats,c(Rolling_ME_dataset_Arima113_Seasoanl212,Rolling_MSE_dataset_Arima113_Seasoanl212,Rolling_MAE_dataset_Arima113_Seasoanl212,Rolling_MAPE_dataset_Arima113_Seasoanl212,Rolling_RMSE_dataset_Arima113_Seasoanl212))
Summary_stats<- rbind(Summary_stats,c(Arima302_Seasoanl112_dataset_ME,Arima302_Seasoanl112_dataset_MSE,Arima302_Seasoanl112_dataset_MAE,Arima302_Seasoanl112_dataset_MAPE,Arima302_Seasoanl112_dataset_RMSE))
Summary_stats<- rbind(Summary_stats,c(Rolling_ME_dataset_Arima302_Seasoanl112,Rolling_MSE_dataset_Arima302_Seasoanl112,Rolling_MAE_dataset_Arima302_Seasoanl112,Rolling_MAPE_dataset_Arima302_Seasoanl112,Rolling_RMSE_dataset_Arima302_Seasoanl112))
Summary_stats<- rbind(Summary_stats,c(Arima402_Seasoanl311_dataset_ME,Arima402_Seasoanl311_dataset_MSE,Arima402_Seasoanl311_dataset_MAE,Arima402_Seasoanl311_dataset_MAPE,Arima402_Seasoanl311_dataset_RMSE))
Summary_stats<- rbind(Summary_stats,c(Rolling_ME_dataset_Arima402_Seasoanl311,Rolling_MSE_dataset_Arima402_Seasoanl311,Rolling_MAE_dataset_Arima402_Seasoanl311,Rolling_MAPE_dataset_Arima402_Seasoanl311,Rolling_RMSE_dataset_Arima402_Seasoanl311))

colnames(Summary_stats) <- c('ME','MSE','MAE','MAPE','RMSE')
rownames(Summary_stats) <- c('Arithmetic Mean Model Error Measures','Arithmetic Mean Model Error Measures with Rolling origin','Simple Moving Average Model Error Measures','Simple Moving Average Model Error Measures with Rolling origin', 'Seasonal Naive Model Error Measures', 'Seasonal Naive Model Error Measures with Rolling origin','ETS ANA Model Error Measures','ETS ANA Model Error Measures with Rolling origin','ETS AAA Model Error Measures','ETS AAA Model Error Measures with Rolling origin','Arima711_Seasoanl013 Model Error Measures', 'Arima711_Seasoanl013 Model Error Measures with Rolling origin','Arima202 Seasoanl012 Model Error Measures','Arima202 Seasoanl012 Model Error Measures with Rolling origin','Arima 202 Seasoanl 111 Model Error Measures','Arima 202 Seasoanl 111 Model Error Measures with Rolling origin','Arima 113 Seasoanl 212 Model Error Measures','Arima 113 Seasoanl 212 Model Error Measures with Rolling origin','Arima 302 Seasoanl 112 Model Error Measures','Arima 302 Seasoanl 112 Model Error Measures with Rolling origin
','Arima 402 Seasoanl 311 Model Error Measures','Arima 402 Seasoanl 311 Model Error Measures with Rolling origin')
Summary_stats <- as.table(Summary_stats)
names(Summary_stats) <- c("Arithmetic Mean Model Error Measures","Arithmetic Mean Model Error Measures with Rolling origin","Simple Moving Average Model Error Measures","Simple Moving Average Model Error Measures with Rolling origin","Seasonal Naive Model Error Measures","Seasonal Naive Model Error Measures with Rolling origin","ETS ANA Model Error Measures","ETS ANA Model Error Measures with Rolling origin","ETS AAA Model Error Measures","ETS AAA Model Error Measures with Rolling origin","Arima711_Seasoanl013 Model Error Measures","Arima711_Seasoanl013 Model Error Measures with Rolling origin","Arima202 Seasoanl012 Model Error Measures","Arima202 Seasoanl012 Model Error Measures with Rolling origin","Arima 202 Seasoanl 111 Model Error Measures","Arima 202 Seasoanl 111 Model Error Measures with Rolling origin","Arima 113 Seasoanl 212 Model Error Measures","Arima 113 Seasoanl 212 Model Error Measures with Rolling origin","Arima 302 Seasoanl 112 Model Error Measures","Arima 302 Seasoanl 112 Model Error Measures with Rolling origin
","Arima 402 Seasoanl 311 Model Error Measures","Arima 402 Seasoanl 311 Model Error Measures with Rolling origin")
knitr::kable(Summary_stats)
```

#### Model selection


## Regression

If we add L7 lag value the value of adjusted r-square is increasingly significantly along with L1 and L2 lags dataset. If check the VIF factor its coming under 5 which means good to go. Now we will check how can we improve the model by adding the seasonal dummies. For now if we keep all the seasonal dummies we created as Mon to Sat in the regression model just created by using lags dataset , the adjusted R squared values has increased to 0.4132.But this could add the multi-collinearity effect as well. If remove the variables which could be having similar significance, it may become the better model. Hence by doing this, we remove the L3, L5 and L6 lag value from the model. Now the new model consist of L1, L2, L4, L7 lag values along with 6 seasonal dummies of day of week/. When checked the p-value for all the variables, p-value for Mon and Tue variable is higher than the 5% significance level and hence we will remove the Mon and Tue variable. This model is giving adjusted R squared value as 0.4157. If you add the trend variable into the model, it will increase further the adjusted R squared value to 0.4262 and the p-values for all the variables are falling under 5  % significance level. If checked the VIF factor, all are coming under the 6 which means we are not having multicollinearity effect.
The model which could be the good fit is (Transactions ~ L1_dataset + L2_dataset + L4_dataset + L7_dataset +   Wed  + Thu + Fri + Sat + dataset_trend). This has AIC as 4271.513 and BIC is 4321.792. Also, adjusted R squared value is 0.4262. 
Automatically built model by the lm function in "R" is giving the model as (Transactions ~ L1_dataset + L2_dataset + L4_dataset + L7_dataset +  L12_dataset + L14_dataset + Wed + Thu + Fri + Sat ) when we run the algorithm in both direction, i.e. Forward and Backward. This model has AIC value as 4241.827 and BIC value 4296.559 which is quite lower than the identified model as above. But the Adjusted R-squared:  0.4146  has lower value than the above model which could be downside to this model.

Hence the good fit candidate for the regression model is (Transactions ~ L1_dataset + L2_dataset + L4_dataset + L7_dataset  + Wed + Thu + Fri + Sat ) where Transactions is the dataset time series and L1, L2, L4 and L7 are the lag variable of the dataset along with Wed, Thu, Fri and Sat are the dummy variables for the day of week.

```{r Neural Network, include=FALSE}

#Neural Network Auto regression
ann1 <-nnetar(dataset_train)

# Check the summary
summary(ann1)

# Check the ACF/PACF plot of residuals
tsdisplay(residuals(ann1))

# Forecast using NN
accnfcst<-forecast(ann1,h=h)
accnfcst
# Plot the forecast
autoplot(accnfcst)

#We create a simulation matrix to support 9 different outputs.
sim <- ts(matrix(0, nrow=12L, ncol=9L),start = end(dataset_train)[1L]+1L, frequency = 7)
#Simulate 9 possible future sample paths using bootstrapping. You will get a warning related to the 
#column names, just ignore it: 
for(i in seq(9))
  sim[,i] <- simulate(ann1, nsim=12)

autoplot(dataset_train) + autolayer(sim)

fcast <- forecast(ann1, PI=TRUE, h=14)
autoplot(fcast)

#Error measures for NN
NN_errors_dataset = dataset_test - (accnfcst$mean)
NN_ME_dataset <- mean(NN_errors_dataset) #Mean error
NN_MSE_dataset <- mean(NN_errors_dataset^2) #Mean squared error
NN_MAE_dataset <- mean(abs(NN_errors_dataset)) #Mean absolute error
NN_MAPE_dataset <- 100 * mean(abs(NN_errors_dataset)/dataset_test) #Mean absolute percentage error
NN_RMSE_dataset <- sqrt(mean(NN_errors_dataset^2)) #Root mean square

```

```{r Simple Regression}

#Import sdata
dataset <- read_excel("Assignment 2 Data.xls")

#colnames(data) <- c("Date","Transactions")
colnames(dataset) <- c("Transactions")

#Converting data to time series
dataset <- ts(dataset, frequency = 365, start = c(1996,77))

# Approach 1 -Replace with corresponding day of week data
which_na(dataset)

# Approach 2 - Replace with interpolation method
dataset <- na_interpolation(dataset)


# Find the outliers in the series
out <- boxplot.stats(dataset)$out 
out
out_ind <- which(dataset %in% c(out))
out_ind
dataset[c(163,167,196,235,265,283,538,557,558,559,561,562,563,565,586,587,592,593,594,600
                 ,621,628,649,670,684,691,698,711,712,726)]

#Replacing the outliers with median values 
series_median = median(dataset)
series_median
dataset[c(163,167,196,235,265,283,538,557,558,559,561,562,563,565,586,587,592,593,594,600
                 ,621,628,649,670,684,691,698,711,712,726)] = series_median

# Split the data into train and test sets
dataset_train <- window(dataset, start(dataset), (1998+66/365))
dataset_train
# Split the data into train and test sets
dataset_test <- window(dataset, (1998+67/365), end(dataset))
dataset_test

# Fit Simple Regression
Simple_Regression <- lm(Transactions ~ 1  , data=dataset_train)
# Summary
summary(Simple_Regression)
#Extract Residuals
Simple_Regression_residuals <- residuals(Simple_Regression)
#We will also need fitted values for our analysis, which can be extracted using fitted():
#Extract Residuals
Simple_Regression_fitted <- fitted(Simple_Regression)
#Plot Histogram
hist(Simple_Regression_residuals)
#QQ-Plot
qqnorm(Simple_Regression_residuals)
qqline(Simple_Regression_residuals)
#Jarque-Bera test
jarque.bera.test(Simple_Regression_residuals)
#Shapiro-Wilk test
shapiro.test(Simple_Regression_residuals)
#Kolmogorov-Smirnov test
ks.test(Simple_Regression_residuals,y="rnorm")
#Plot Residuals against Fitted Values
plot(Simple_Regression_fitted, Simple_Regression_residuals)
#Plot Residuals against Fitted Values
plot(Simple_Regression_fitted, Simple_Regression_residuals^2)
#ACF and PACF of the residuals
tsdisplay(Simple_Regression_residuals)

# Create Studentised Residuals
Simple_Regression_st <- rstandard(Simple_Regression)
# Plot the Residuals
plot(Simple_Regression_st)
# Draw two horizontal lines at 2 and -2 in red
abline(h=c(-2,2),col="red")

#Forecast from Simple Regression
Forecast_Simple_Regression <- predict(Simple_Regression, (as.data.frame(dataset_test)))
Forecast_Simple_Regression

#Error measures for Simple Regression
Simple_Regression_errors_dataset = dataset_test - Forecast_Simple_Regression
Simple_Regression_ME_dataset <- mean(Simple_Regression_errors_dataset) #Mean error
Simple_Regression_MSE_dataset <- mean(Simple_Regression_errors_dataset^2) #Mean squared error
Simple_Regression_MAE_dataset <- mean(abs(Simple_Regression_errors_dataset)) #Mean absolute error
Simple_Regression_MAPE_dataset <- 100 * mean(abs(Simple_Regression_errors_dataset)/dataset_test) #Mean absolute percentage error
Simple_Regression_RMSE_dataset <- sqrt(mean(Simple_Regression_errors_dataset^2)) #Root mean squared error

```

```{r Multiple Regression Model Parameters, include=FALSE}
# Add dummy variables for lag and seasonal variables

# Auto regressive model with only lag - seasonal variable

# The second one assumes that the
#seasonality has a stochastic structure (implying that it may change over time) and uses lagged
#variables.

#Lags of dataset

L1_dataset <- lag((as.vector(dataset)),k=1)
L1_dataset
L2_dataset <- lag((as.vector(L1_dataset)),k=1)
L2_dataset
L3_dataset <- lag((as.vector(L2_dataset)),k=1)
L3_dataset
L4_dataset <- lag((as.vector(L3_dataset)),k=1)
L4_dataset
L5_dataset <- lag((as.vector(L4_dataset)),k=1)
L5_dataset
L6_dataset <- lag((as.vector(L5_dataset)),k=1)
L6_dataset
L7_dataset <- lag((as.vector(L6_dataset)),k=1)
L7_dataset
L8_dataset <- lag((as.vector(L7_dataset)),k=1)
L8_dataset
L9_dataset <- lag((as.vector(L8_dataset)),k=1)
L9_dataset
L10_dataset <- lag((as.vector(L9_dataset)),k=1)
L10_dataset
L11_dataset <- lag((as.vector(L10_dataset)),k=1)
L11_dataset
L12_dataset <- lag((as.vector(L11_dataset)),k=1)
L12_dataset
L13_dataset <- lag((as.vector(L12_dataset)),k=1)
L13_dataset
L14_dataset <- lag((as.vector(L13_dataset)),k=1)
L14_dataset


#Add all Lags to the Data
dataset_colnames <- colnames(dataset)
dataset <- cbind(dataset, L1_dataset, L2_dataset, L3_dataset, L4_dataset, L5_dataset, L6_dataset, L7_dataset, L8_dataset, L9_dataset, L10_dataset, L11_dataset, L12_dataset, L13_dataset, L14_dataset)

# Change the column names
colnames(dataset) <- c("Transactions", "L1_dataset", "L2_dataset", "L3_dataset", "L4_dataset", "L5_dataset", "L6_dataset", "L7_dataset", "L8_dataset", "L9_dataset", "L10_dataset", "L11_dataset", "L12_dataset", "L13_dataset", "L14_dataset")

# Add the seasonal dummies

#Create Seasonal Dummies
Mon <- rep(c(1,0,0,0,0,0,0),105)
Tue <- rep(c(0,1,0,0,0,0,0),105)
Wed <- rep(c(0,0,1,0,0,0,0),105)
Thu <- rep(c(0,0,0,1,0,0,0),105)
Fri <- rep(c(0,0,0,0,1,0,0),105)
Sat <- rep(c(0,0,0,0,0,1,0),105)
Sun <- rep(c(0,0,0,0,0,0,1),105)

# Add the seasonal dummies to the dataset
dataset <- cbind(dataset,Mon,Tue,Wed,Thu,Fri, Sat, Sun)

# Change the column names
colnames(dataset) <- c("Transactions", "L1_dataset", "L2_dataset", "L3_dataset", "L4_dataset", "L5_dataset", "L6_dataset", "L7_dataset", "L8_dataset", "L9_dataset", "L10_dataset", "L11_dataset", "L12_dataset", "L13_dataset", "L14_dataset","Mon","Tue","Wed","Thu", "Fri", "Sat", "Sun")

# Add the trend dummy

## Create Trend
dataset_trend <- c(1:735)

#Add trend to the Data
data_colnames <- colnames(dataset)
dataset <- cbind(dataset, dataset_trend)

# Change the column names
colnames(dataset) <- c("Transactions", "L1_dataset", "L2_dataset", "L3_dataset", "L4_dataset", "L5_dataset", "L6_dataset", "L7_dataset", "L8_dataset", "L9_dataset", "L10_dataset", "L11_dataset", "L12_dataset", "L13_dataset", "L14_dataset","Mon","Tue","Wed","Thu", "Fri", "Sat", "Sun", "dataset_trend")


# Split the data into train and test sets
dataset_train <- window(dataset, start(dataset), (1998+66/365))
dataset_train
# Split the data into train and test sets
dataset_test <- window(dataset, (1998+67/365), end(dataset))
dataset_test

```

```{r Regression model with lags Model, include=FALSE}

# Model
lags_model <- lm(Transactions ~   L1_dataset + L7_dataset, data=dataset_train)

# L1_dataset + L2_dataset + L4_dataset + L7_dataset
summary(lags_model)

tsdisplay(residuals(lags_model)) 

# Check for multi collinearity
#VIF for fit4
 VIF(lags_model)

#Extract Residuals
lags_model_residuals <- residuals(lags_model)
#We will also need fitted values for our analysis, which can be extracted using fitted():
#Extract Residuals
lags_model_fitted <- fitted(lags_model)
#Plot Histogram
hist(lags_model_residuals)
#QQ-Plot
qqnorm(lags_model_residuals)
qqline(lags_model_residuals)
#Jarque-Bera test
jarque.bera.test(lags_model_residuals)
#Shapiro-Wilk test
shapiro.test(lags_model_residuals)
#Kolmogorov-Smirnov test
ks.test(lags_model_residuals,y="rnorm")
#Plot Residuals against Fitted Values
plot(lags_model_fitted, lags_model_residuals)
#Plot Residuals against Fitted Values
plot(lags_model_fitted, lags_model_residuals^2)
#ACF and PACF of the residuals
tsdisplay(lags_model_residuals)

#Forecast from lags_model
Forecast_lags_model <- predict(lags_model, (as.data.frame(dataset_test)))
plot(Forecast_lags_model)
Forecast_lags_model

#Error measures for lags_model
lags_model_errors_dataset = dataset_test - Forecast_lags_model
lags_model_ME_dataset <- mean(lags_model_errors_dataset) #Mean error
lags_model_MSE_dataset <- mean(lags_model_errors_dataset^2) #Mean squared error
lags_model_MAE_dataset <- mean(abs(lags_model_errors_dataset)) #Mean absolute error
lags_model_MAPE_dataset <- 100 * mean(abs(lags_model_errors_dataset)/dataset_test) #Mean absolute percentage error
lags_model_RMSE_dataset <- sqrt(mean(lags_model_errors_dataset^2)) #Root mean squared error

```

```{r Regression model with Seasonal dummies Model, include=FALSE}
library("greybox")
#Use the Seasonal Dummies
seasonaldummies <- alm(Transactions ~  Mon + Tue +  Wed + Thu + Fri + Sat  , data=dataset_train)
summary(seasonaldummies)
dataset_train

#Extract Residuals
seasonaldummies_residuals <- residuals(seasonaldummies)
#We will also need fitted values for our analysis, which can be extracted using fitted():
#Extract Residuals
seasonaldummies_fitted <- fitted(seasonaldummies)
#Plot Histogram
hist(seasonaldummies_residuals)
#QQ-Plot
qqnorm(seasonaldummies_residuals)
qqline(seasonaldummies_residuals)
#Jarque-Bera test
jarque.bera.test(seasonaldummies_residuals)
#Shapiro-Wilk test
shapiro.test(seasonaldummies_residuals)
#Kolmogorov-Smirnov test
ks.test(seasonaldummies_residuals,y="rnorm")
#Plot Residuals against Fitted Values
plot(seasonaldummies_fitted, seasonaldummies_residuals)
#Plot Residuals against Fitted Values
plot(seasonaldummies_fitted, seasonaldummies_residuals^2)
#ACF and PACF of the residuals
tsdisplay(seasonaldummies_residuals)

#Forecast from seasonal dummies
Forecast_seasonaldummies <- predict(seasonaldummies, dataset_test)
plot(Forecast_seasonaldummies)
Forecast_seasonaldummies


#Error measures for seasonal dummies
seasonaldummies_errors_dataset = dataset_test - Forecast_seasonaldummies$mean
seasonaldummies_ME_dataset <- mean(seasonaldummies_errors_dataset) #Mean error
seasonaldummies_MSE_dataset <- mean(seasonaldummies_errors_dataset^2) #Mean squared error
seasonaldummies_MAE_dataset <- mean(abs(seasonaldummies_errors_dataset)) #Mean absolute error
seasonaldummies_MAPE_dataset <- 100 * mean(abs(seasonaldummies_errors_dataset)/dataset_test) #Mean absolute percentage error
seasonaldummies_RMSE_dataset <- sqrt(mean(seasonaldummies_errors_dataset^2)) #Root mean squared error

```

```{r Auto regressive model with lag, include=FALSE}

Lag_seasonal <- lm(Transactions ~ L1_dataset + L2_dataset + L4_dataset + L7_dataset + Mon + Tue + Wed  + Thu + Fri + Sat , data=dataset_train)

summary(Lag_seasonal)

tsdisplay(residuals(Lag_seasonal)) 

# Check for multi collinearity
#VIF for fit4

VIF(Lag_seasonal)

#Extract Residuals
Lag_seasonal_residuals <- residuals(Lag_seasonal)
#We will also need fitted values for our analysis, which can be extracted using fitted():
#Extract Residuals
Lag_seasonal_fitted <- fitted(Lag_seasonal)
#Plot Histogram
hist(Lag_seasonal_residuals)
#QQ-Plot
qqnorm(Lag_seasonal_residuals)
qqline(Lag_seasonal_residuals)
#Jarque-Bera test
jarque.bera.test(Lag_seasonal_residuals)
#Shapiro-Wilk test
shapiro.test(Lag_seasonal_residuals)
#Kolmogorov-Smirnov test
ks.test(Lag_seasonal_residuals,y="rnorm")
#Plot Residuals against Fitted Values
plot(Lag_seasonal_fitted, Lag_seasonal_residuals)
#Plot Residuals against Fitted Values
plot(Lag_seasonal_fitted, Lag_seasonal_residuals^2)
#ACF and PACF of the residuals
tsdisplay(Lag_seasonal_residuals)

#Forecast from Lag Seasonal
Forecast_Lag_seasonal <- predict(Lag_seasonal, (as.data.frame(dataset_test)))
plot(Forecast_Lag_seasonal)
Forecast_Lag_seasonal

#Error measures for Lag Seasonal
Lag_seasonal_errors_dataset = dataset_test - Forecast_Lag_seasonal
Lag_seasonal_ME_dataset <- mean(Lag_seasonal_errors_dataset) #Mean error
Lag_seasonal_MSE_dataset <- mean(Lag_seasonal_errors_dataset^2) #Mean squared error
Lag_seasonal_MAE_dataset <- mean(abs(Lag_seasonal_errors_dataset)) #Mean absolute error
Lag_seasonal_MAPE_dataset <- 100 * mean(abs(Lag_seasonal_errors_dataset)/dataset_test) #Mean absolute percentage error
Lag_seasonal_RMSE_dataset <- sqrt(mean(Lag_seasonal_errors_dataset^2)) #Root mean squared error

```

```{r Auto regressive model Lag_seasonal_trend, eval=FALSE, include=FALSE}

Lag_seasonal_trend <- lm(Transactions ~ L1_dataset + L2_dataset + L4_dataset + L7_dataset  + Wed  + Thu + Fri + Sat + dataset_trend , data=dataset_train)

summary(Lag_seasonal_trend)

tsdisplay(residuals(Lag_seasonal_trend)) 

# Check for multi collinearity
#VIF for fit4

VIF(Lag_seasonal_trend)

#Extract Residuals
Lag_seasonal_trend_residuals <- residuals(Lag_seasonal_trend)
#We will also need fitted values for our analysis, which can be extracted using fitted():
#Extract Residuals
Lag_seasonal_trend_fitted <- fitted(Lag_seasonal_trend)
#Plot Histogram
hist(Lag_seasonal_trend_residuals)
#QQ-Plot
qqnorm(Lag_seasonal_trend_residuals)
qqline(Lag_seasonal_trend_residuals)
#Jarque-Bera test
jarque.bera.test(Lag_seasonal_trend_residuals)
#Shapiro-Wilk test
shapiro.test(Lag_seasonal_trend_residuals)
#Kolmogorov-Smirnov test
ks.test(Lag_seasonal_trend_residuals,y="rnorm")
#Plot Residuals against Fitted Values
plot(Lag_seasonal_trend_fitted, Lag_seasonal_trend_residuals)
#Plot Residuals against Fitted Values
plot(Lag_seasonal_trend_fitted, Lag_seasonal_trend_residuals^2)
#ACF and PACF of the residuals
tsdisplay(Lag_seasonal_trend_residuals)

#Forecast from Lag Seasonal Trend
Forecast_Lag_seasonal_trend <- predict(Lag_seasonal_trend, (as.data.frame(dataset_test)))
plot(Forecast_Lag_seasonal_trend)
Forecast_Lag_seasonal_trend

#Error measures for Lag Seasonal Trend
Lag_seasonal_trend_errors_dataset = dataset_test - Forecast_Lag_seasonal_trend
Lag_seasonal_trend_ME_dataset <- mean(Lag_seasonal_trend_errors_dataset) #Mean error
Lag_seasonal_trend_MSE_dataset <- mean(Lag_seasonal_trend_errors_dataset^2) #Mean squared error
Lag_seasonal_trend_MAE_dataset <- mean(abs(Lag_seasonal_trend_errors_dataset)) #Mean absolute error
Lag_seasonal_trend_MAPE_dataset <- 100 * mean(abs(Lag_seasonal_trend_errors_dataset)/dataset_test) #Mean absolute percentage error
Lag_seasonal_trend_RMSE_dataset <- sqrt(mean(Lag_seasonal_trend_errors_dataset^2)) #Root mean squared error

```

```{r ACF and PACF of the residuals of Lag Seasonal Trend Model, eval=FALSE, include=FALSE}
#ACF and PACF of the residuals
tsdisplay(residuals(Lag_seasonal_trend)) 
```

```{r Model with AIC both selection, eval=FALSE, include=FALSE}

all_variable <- lm(Transactions ~ . , data=dataset_train)

AICSelection_directionmodel <- step (all_variable, direction = "both")

summary(AICSelection_directionmodel)

tsdisplay(residuals(AICSelection_directionmodel)) 

# Check for multi collinearity
#VIF for fit4

VIF(AICSelection_directionmodel)

#Extract Residuals
AICSelection_directionmodel_residuals <- residuals(AICSelection_directionmodel)
#We will also need fitted values for our analysis, which can be extracted using fitted():
#Extract Residuals
AICSelection_directionmodel_fitted <- fitted(AICSelection_directionmodel)
#Plot Histogram
hist(AICSelection_directionmodel_residuals)
#QQ-Plot
qqnorm(AICSelection_directionmodel_residuals)
qqline(AICSelection_directionmodel_residuals)
#Jarque-Bera test
jarque.bera.test(AICSelection_directionmodel_residuals)
#Shapiro-Wilk test
shapiro.test(AICSelection_directionmodel_residuals)
#Kolmogorov-Smirnov test
ks.test(AICSelection_directionmodel_residuals,y="rnorm")
#Plot Residuals against Fitted Values
plot(AICSelection_directionmodel_fitted, AICSelection_directionmodel_residuals)
#Plot Residuals against Fitted Values
plot(AICSelection_directionmodel_fitted, AICSelection_directionmodel_residuals^2)
#ACF and PACF of the residuals
tsdisplay(AICSelection_directionmodel_residuals)

# Forecast using AIC selection model
Forecast_AICSelection_directionmodel <- predict(AICSelection_directionmodel, dataset_test)

plot(Forecast_AICSelection_directionmodel)
Forecast_AICSelection_directionmodel

#Error measures for Lag Seasonal Trend
AICSelection_directionmodel_errors_dataset = dataset_test - Forecast_AICSelection_directionmodel
AICSelection_directionmodel_ME_dataset <- mean(AICSelection_directionmodel_errors_dataset) #Mean error
AICSelection_directionmodel_MSE_dataset <- mean(AICSelection_directionmodel_errors_dataset^2) #Mean squared error
AICSelection_directionmodel_MAE_dataset <- mean(abs(AICSelection_directionmodel_errors_dataset)) #Mean absolute error
AICSelection_directionmodel_MAPE_dataset <- 100 * mean(abs(AICSelection_directionmodel_errors_dataset)/dataset_test) #Mean absolute percentage error
AICSelection_directionmodel_RMSE_dataset <- sqrt(mean(AICSelection_directionmodel_errors_dataset^2)) #Root mean squared error

```

## Model selection

For the dataset, we critically analysed the different models as follows: Arithmatic Mean, Simle Moving Average, Naive, Seasonal Naive, ETS ANA, ETS AAA, ETS MAM, ETS MNA, ETS Optimised, ARIMA(7,1,1)(0,1,3)[7],ARIMA(2,0,2)(0,1,2)[7],ARIMA(2,0,2)(1,1,1)[7],ARIMA(1,1,3)(2,1,2)[7],ARIMA(3,0,2)(1,1,2)[7], ARIMA(4,0,2)(3,1,1)[7], Auto ARIMA, Simple Regression, Multiple regression with lag variables, Multiple regression with seasonal dummies variables, Multiple regression with lag and seasonal dummies variables, Multiple regression with lag, seasonal dummies and trend variables. 
Based on the output measured against the test dataset, error matrix final recommended models are :
ARIMA(3,0,2)(1,1,2)[7] and Multiple regression with lag, seasonal dummies and trend model.


## Conclusion



## Executive Summary

For the dataset, we applied proven forecasting models to forecast the 2 weeks data for the ATM transactions in the given dataset. Below is the forecasted values for them .Wehen analysed the data, most of the transactions are in between 17 to 26. The maximum transaction happened was 69  but there are very few numbers having such high transactions. After we applied the forecasting models to the dataset the transactions for this 2 weeks data are in the range of 17 to 26 based on the forecasting model output which is in line with the actual dataset. Also would like to comment over the part where for some days of the week there are high number of transactions occurred for ex. Wed, Thud, Fri, Sat. Also, during some specific months the number of transactions were higher than the usual transaction this means that more cash had been withdrawn on those specific months. The dataset has 735 observations and out of those 20 were the missing observations, out of 20, 12 were from specific day of week which is Sat. 
While running the different forecasting models, I created some parameters which will take care of the day of the week frequency. Hence I critically analysed the significance related to the day of the week as well. The predicted values for next 2 weeks singnies that this many transactions would kiley to happen in this 2 weeks. There would be not be significanlty difference with actual transactions. In the 95 percentages of cases this transactions are likely to be happened.



## References

## Appendix

